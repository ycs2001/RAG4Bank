"""
ç»Ÿä¸€çš„RAGç³»ç»Ÿ - æ•´åˆæ‰€æœ‰åŠŸèƒ½ï¼Œé…ç½®é©±åŠ¨ï¼Œç®€åŒ–æ¶æ„
"""

import logging
import time
from typing import Dict, Any, List, Optional
from dataclasses import dataclass
from pathlib import Path

from ..config import ConfigManager
from ..llm.deepseek_llm import DeepSeekLLM
from ..retrievers.chromadb_retriever import ChromaDBRetriever
from ..rerankers import BaseReranker, CrossEncoderReranker


@dataclass
class RAGResponse:
    """RAGå“åº”ç»“æœ"""
    answer: str
    retrieval_count: int
    processing_time: float
    collections_used: List[str]
    metadata: Dict[str, Any]


class TopicClassifier:
    """ç®€åŒ–çš„ä¸»é¢˜åˆ†ç±»å™¨"""
    
    def __init__(self, config_manager: ConfigManager):
        self.config_manager = config_manager
        self.logger = logging.getLogger(__name__)
        
        # åŠ è½½é›†åˆé…ç½®
        self.collections_config = config_manager.get('retrieval.collections', [])
        
        # æ„å»ºå…³é”®è¯æ˜ å°„
        self.keyword_mapping = {}
        for collection in self.collections_config:
            collection_id = collection['collection_id']
            keywords = collection.get('keywords', [])
            self.keyword_mapping[collection_id] = keywords
    
    def classify(self, query: str) -> List[str]:
        """åˆ†ç±»æŸ¥è¯¢ï¼Œè¿”å›æ¨èçš„é›†åˆIDåˆ—è¡¨"""
        query_lower = query.lower()
        matched_collections = []

        # æ£€æµ‹æ¯”è¾ƒæŸ¥è¯¢å…³é”®è¯
        comparison_keywords = ['å¯¹æ¯”', 'æ¯”è¾ƒ', 'å·®å¼‚', 'å·®åˆ«', 'åŒºåˆ«', 'ç›¸æ¯”']

        is_comparison = (
            any(keyword in query_lower for keyword in comparison_keywords) or
            ('å’Œ' in query_lower and ('åˆ†æ' in query_lower or 'å¯¹æ¯”' in query_lower or 'æ¯”è¾ƒ' in query_lower))
        )

        # æ£€æµ‹æ¶‰åŠçš„ç³»ç»Ÿ
        has_east = 'east' in query_lower
        has_ybt = 'ä¸€è¡¨é€š' in query_lower
        has_pboc = any(keyword in query_lower for keyword in ['äººæ°‘é“¶è¡Œ', 'å¤®è¡Œ', 'é‡‘èç»Ÿè®¡', 'å¤§é›†ä¸­'])
        has_1104 = any(keyword in query_lower for keyword in ['1104', 's71', 'g01'])

        # æ¯”è¾ƒæŸ¥è¯¢ï¼šåŒ…å«å¤šä¸ªç³»ç»Ÿ
        if is_comparison:
            if has_pboc and has_1104:
                matched_collections = ['pboc_statistics', 'report_1104_2024']
                self.logger.info(f"ğŸ”„ æ£€æµ‹åˆ°äººæ°‘é“¶è¡Œä¸1104æŠ¥è¡¨æ¯”è¾ƒæŸ¥è¯¢")
            elif has_east and has_1104:
                matched_collections = ['east_data_structure', 'east_metadata', 'report_1104_2024']
                self.logger.info(f"ğŸ”„ æ£€æµ‹åˆ°EASTä¸1104æŠ¥è¡¨æ¯”è¾ƒæŸ¥è¯¢")
            elif has_ybt and has_1104:
                matched_collections = ['ybt_data_structure', 'ybt_product_mapping', 'report_1104_2024']
                self.logger.info(f"ğŸ”„ æ£€æµ‹åˆ°ä¸€è¡¨é€šä¸1104æŠ¥è¡¨æ¯”è¾ƒæŸ¥è¯¢")
            elif has_1104 and ('2024' in query_lower and '2022' in query_lower):
                matched_collections = ['report_1104_2024', 'report_1104_2022']
                self.logger.info(f"ğŸ”„ æ£€æµ‹åˆ°1104æŠ¥è¡¨ç‰ˆæœ¬æ¯”è¾ƒæŸ¥è¯¢")
            else:
                # é€šç”¨æ¯”è¾ƒæŸ¥è¯¢ï¼Œä½¿ç”¨å¤šä¸ªç›¸å…³é›†åˆ
                if has_pboc:
                    matched_collections.append('pboc_statistics')
                if has_1104:
                    matched_collections.extend(['report_1104_2024', 'report_1104_2022'])
                if has_east:
                    matched_collections.extend(['east_data_structure', 'east_metadata'])
                if has_ybt:
                    matched_collections.extend(['ybt_data_structure', 'ybt_product_mapping'])
                self.logger.info(f"ğŸ”„ æ£€æµ‹åˆ°æ¯”è¾ƒæŸ¥è¯¢ï¼Œä½¿ç”¨å¤šé›†åˆæ£€ç´¢")

            if matched_collections:
                return matched_collections[:3]  # é™åˆ¶æœ€å¤š3ä¸ªé›†åˆ

        # å•ä¸€ç³»ç»ŸæŸ¥è¯¢ï¼ˆæ’ä»–æ€§åŒ¹é…ï¼‰
        if has_east:
            matched_collections = ['east_data_structure', 'east_metadata']
            self.logger.info(f"ğŸ¯ æ£€æµ‹åˆ°EASTæŸ¥è¯¢ï¼Œåªä½¿ç”¨EASTé›†åˆ")
            return matched_collections
        elif has_ybt:
            matched_collections = ['ybt_data_structure', 'ybt_product_mapping']
            self.logger.info(f"ğŸ¯ æ£€æµ‹åˆ°ä¸€è¡¨é€šæŸ¥è¯¢ï¼Œåªä½¿ç”¨ä¸€è¡¨é€šé›†åˆ")
            return matched_collections
        elif has_pboc:
            matched_collections = ['pboc_statistics']
            self.logger.info(f"ğŸ¯ æ£€æµ‹åˆ°äººæ°‘é“¶è¡ŒæŸ¥è¯¢ï¼Œåªä½¿ç”¨äººæ°‘é“¶è¡Œé›†åˆ")
            return matched_collections

        # å…³é”®è¯åŒ¹é…ï¼ˆä½œä¸ºè¡¥å……ï¼‰
        for collection_id, keywords in self.keyword_mapping.items():
            for keyword in keywords:
                if keyword.lower() in query_lower:
                    if collection_id not in matched_collections:
                        matched_collections.append(collection_id)
                    break

        # ç‰¹æ®Šå¤„ç†ï¼šæ™®æƒ é‡‘èç›¸å…³æŸ¥è¯¢ï¼ˆä»…åœ¨æ²¡æœ‰æ˜ç¡®ç³»ç»ŸæŒ‡å‘æ—¶ï¼‰
        if not matched_collections and any(keyword in query_lower for keyword in ['æ™®æƒ é‡‘è', 'æŠ¥é€è¡¨', 'æ¶‰åŠå“ªäº›è¡¨']):
            for collection_id in ['report_1104_2024', 'pboc_statistics']:
                if collection_id not in matched_collections:
                    matched_collections.append(collection_id)

        # å¦‚æœæ²¡æœ‰åŒ¹é…ï¼Œé»˜è®¤ä½¿ç”¨æœ€æ–°çš„1104æŠ¥è¡¨
        if not matched_collections:
            matched_collections = ['report_1104_2024']
            self.logger.info(f"ğŸ”„ æœªåŒ¹é…åˆ°ç‰¹å®šé›†åˆï¼Œä½¿ç”¨é»˜è®¤1104æŠ¥è¡¨")

        # é™åˆ¶æœ€å¤š3ä¸ªé›†åˆ
        return matched_collections[:3]


class UnifiedRAGSystem:
    """ç»Ÿä¸€çš„RAGç³»ç»Ÿ"""
    
    def __init__(self, config_manager: ConfigManager):
        """åˆå§‹åŒ–RAGç³»ç»Ÿ"""
        self.config_manager = config_manager
        self.logger = logging.getLogger(__name__)
        
        # ä»é…ç½®è¯»å–å‚æ•°
        self.top_k = config_manager.get('retrieval.top_k', 10)
        self.similarity_threshold = config_manager.get('retrieval.similarity_threshold', 0.5)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._initialize_components()
        
        self.logger.info("âœ… ç»Ÿä¸€RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"ğŸ“Š é…ç½®: top_k={self.top_k}, threshold={self.similarity_threshold} (æ— å­—ç¬¦é™åˆ¶)")

    def _init_reranker(self):
        """åˆå§‹åŒ–é‡æ’å™¨"""
        try:
            reranker_config = self.config_manager.get('reranker', {})

            if not reranker_config.get('enabled', False):
                self.reranker = None
                self.logger.info("é‡æ’å™¨æœªå¯ç”¨")
                return

            reranker_type = reranker_config.get('type', 'cross_encoder')

            if reranker_type == 'cross_encoder':
                cross_encoder_config = reranker_config.get('cross_encoder', {})
                self.reranker = CrossEncoderReranker(cross_encoder_config)
            else:
                self.logger.warning(f"æœªçŸ¥çš„é‡æ’å™¨ç±»å‹: {reranker_type}")
                self.reranker = None
                return

            if self.reranker and self.reranker.is_enabled():
                self.logger.info(f"âœ… é‡æ’å™¨åˆå§‹åŒ–æˆåŠŸ: {reranker_type}")
            else:
                self.logger.warning("é‡æ’å™¨åˆå§‹åŒ–å¤±è´¥æˆ–è¢«ç¦ç”¨")
                self.reranker = None

        except Exception as e:
            self.logger.error(f"é‡æ’å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.reranker = None
    
    def _initialize_components(self):
        """åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶"""
        # 1. éªŒè¯é…ç½®
        self._validate_configuration()
        
        # 2. åˆå§‹åŒ–LLM
        deepseek_config = self.config_manager.get('llm.deepseek', {})
        self.llm = DeepSeekLLM(deepseek_config)
        
        # 3. åˆå§‹åŒ–æ£€ç´¢å™¨
        retrieval_config = self.config_manager.get_section('retrieval')
        self.retriever = ChromaDBRetriever(retrieval_config)
        
        # 4. åˆå§‹åŒ–ä¸»é¢˜åˆ†ç±»å™¨
        self.topic_classifier = TopicClassifier(self.config_manager)

        # 5. åˆå§‹åŒ–é‡æ’å™¨
        self._init_reranker()

        self.logger.info("âœ… æ‰€æœ‰ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
    
    def _validate_configuration(self):
        """éªŒè¯é…ç½®å®Œæ•´æ€§"""
        # æ£€æŸ¥DeepSeeké…ç½®
        deepseek_config = self.config_manager.get('llm.deepseek', {})
        if not deepseek_config.get('api_key'):
            raise ValueError("DeepSeek APIå¯†é’¥æœªé…ç½®")
        
        # æ£€æŸ¥BGEæ¨¡å‹è·¯å¾„
        embedding_config = self.config_manager.get('retrieval.embedding', {})
        model_path = embedding_config.get('model_path')
        if not model_path or not Path(model_path).exists():
            raise ValueError(f"BGEæ¨¡å‹è·¯å¾„æ— æ•ˆ: {model_path}")
        
        # æ£€æŸ¥ChromaDBè·¯å¾„
        chromadb_config = self.config_manager.get('retrieval.chromadb', {})
        db_path = chromadb_config.get('db_path', './data/chroma_db')
        if not Path(db_path).exists():
            raise ValueError(f"ChromaDBæ•°æ®åº“ä¸å­˜åœ¨: {db_path}")
        
        # æ£€æŸ¥é›†åˆé…ç½®
        collections = self.config_manager.get('retrieval.collections', [])
        if not collections:
            raise ValueError("æœªé…ç½®ä»»ä½•æ–‡æ¡£é›†åˆ")
    
    def answer_question(self, question: str) -> RAGResponse:
        """å›ç­”é—®é¢˜"""
        start_time = time.time()
        
        try:
            self.logger.info(f"ğŸ” å¤„ç†é—®é¢˜: {question}")

            # 1. æŸ¥è¯¢è¯­ä¹‰å¢å¼º
            enhanced_query_info = self._enhance_query(question)

            # 2. ä¸»é¢˜åˆ†ç±»ï¼ˆä½¿ç”¨å¢å¼ºåçš„ä¿¡æ¯ï¼‰
            collections = enhanced_query_info.get('collections', self.topic_classifier.classify(question))
            self.logger.info(f"ğŸ¯ æ¨èé›†åˆ: {collections}")

            # 3. æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼ˆä½¿ç”¨å¢å¼ºçš„å…³é”®è¯ï¼‰
            search_queries = enhanced_query_info.get('keywords', [question])
            retrieval_results = self._enhanced_retrieve(search_queries, collections)
            
            if not retrieval_results:
                raise ValueError("æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
            
            self.logger.info(f"ğŸ“š æ£€ç´¢åˆ° {len(retrieval_results)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ")
            
            # 3. æ„å»ºä¸Šä¸‹æ–‡
            context = self._build_context(retrieval_results)
            
            # 4. ç”Ÿæˆç­”æ¡ˆ
            answer = self._generate_answer(question, context)
            
            # 5. æ„å»ºå“åº”
            processing_time = time.time() - start_time
            
            response = RAGResponse(
                answer=answer,
                retrieval_count=len(retrieval_results),
                processing_time=processing_time,
                collections_used=collections,
                metadata={
                    'retrieval_scores': [r.score for r in retrieval_results[:5]],
                    'context_length': len(context)
                }
            )
            
            self.logger.info(f"âœ… é—®é¢˜å¤„ç†å®Œæˆï¼Œè€—æ—¶ {processing_time:.2f}ç§’")
            return response
            
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = f"å¤„ç†é—®é¢˜æ—¶å‘ç”Ÿé”™è¯¯: {e}"
            self.logger.error(error_msg)
            
            return RAGResponse(
                answer=f"æŠ±æ­‰ï¼Œ{error_msg}",
                retrieval_count=0,
                processing_time=processing_time,
                collections_used=[],
                metadata={'error': str(e)}
            )
    
    def _build_context(self, retrieval_results: List[Any]) -> str:
        """æ„å»ºä¸Šä¸‹æ–‡"""
        context_parts = []
        total_length = 0
        max_total_length = 50000  # ç®€å•çš„æ€»é•¿åº¦é™åˆ¶ï¼Œé¿å…APIé”™è¯¯

        # ä½¿ç”¨æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼Œä½†æ§åˆ¶æ€»é•¿åº¦
        for i, result in enumerate(retrieval_results):
            # ä¼˜å…ˆä½¿ç”¨åŸæ–‡æ¡£åç§°ï¼Œå›é€€åˆ°é›†åˆID
            source_doc = result.metadata.get('source_document', result.metadata.get('collection_id', 'æœªçŸ¥'))
            doc_text = (
                f"æ–‡æ¡£{i+1} (æ¥æº: {source_doc}, ç›¸ä¼¼åº¦: {result.score:.3f}):\n"
                f"{result.content}\n"
            )

            # æ£€æŸ¥æ˜¯å¦ä¼šè¶…è¿‡é•¿åº¦é™åˆ¶
            if total_length + len(doc_text) > max_total_length:
                self.logger.info(f"ğŸ“ è¾¾åˆ°é•¿åº¦é™åˆ¶ï¼Œä½¿ç”¨å‰{i}ä¸ªæ–‡æ¡£")
                break

            context_parts.append(doc_text)
            total_length += len(doc_text)

        context = "\n".join(context_parts)
        self.logger.info(f"ğŸ“ æ„å»ºä¸Šä¸‹æ–‡: {len(context_parts)}ä¸ªæ–‡æ¡£, æ€»é•¿åº¦{len(context)}å­—ç¬¦")
        return context
    
    def _generate_answer(self, question: str, context: str) -> str:
        """ç”Ÿæˆç­”æ¡ˆ"""
        prompt = f"""
åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹ï¼Œå›ç­”ç”¨æˆ·é—®é¢˜ã€‚è¯·ç¡®ä¿ç­”æ¡ˆå‡†ç¡®ã€ä¸“ä¸šï¼Œå¹¶å¼•ç”¨å…·ä½“çš„æ–‡æ¡£åç§°å’Œæ¡æ¬¾ã€‚

é—®é¢˜: {question}

ç›¸å…³æ–‡æ¡£å†…å®¹:
{context}

è¦æ±‚:
1. åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”
2. å¼•ç”¨ä¿¡æ¯æ—¶å¿…é¡»ä½¿ç”¨æ–‡æ¡£çš„åŸå§‹åç§°ï¼ˆå¦‚"æ ¹æ®ã€Š1104æŠ¥è¡¨åˆè¾‘ã€2022ç‰ˆã€‘.docxã€‹"ã€"æ ¹æ®ã€ŠEASTæ•°æ®ç»“æ„.xlsxã€‹"ç­‰ï¼‰ï¼Œè¿™äº›åç§°åœ¨æ¯ä¸ªæ–‡æ¡£ç‰‡æ®µçš„"æ¥æº"å­—æ®µä¸­æä¾›
3. ä¸è¦ä½¿ç”¨"æ–‡æ¡£1"ã€"æ–‡æ¡£2"ç­‰æ¨¡ç³Šè¡¨è¿°ï¼Œè€Œè¦ä½¿ç”¨å…·ä½“çš„åŸæ–‡æ¡£åç§°
4. å¦‚æœæ¶‰åŠæŠ¥è¡¨ï¼Œè¯·åˆ—å‡ºå…·ä½“çš„è¡¨æ ¼ç¼–å·ï¼ˆå¦‚G01ã€S71ç­‰ï¼‰
5. å¦‚æœæ¶‰åŠç›‘ç®¡è¦æ±‚ï¼Œè¯·å¼•ç”¨å…·ä½“æ¡æ¬¾å’Œæ–‡æ¡£æ¥æº
6. ç­”æ¡ˆè¦ç®€æ´æ˜äº†ï¼Œé‡ç‚¹çªå‡º
7. å¦‚æœæ–‡æ¡£å†…å®¹ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·æ˜ç¡®è¯´æ˜

ç­”æ¡ˆ:
"""

        try:
            response = self.llm.generate(
                prompt, 
                max_tokens=2000, 
                temperature=0
            )
            return response.strip()
        except Exception as e:
            raise RuntimeError(f"ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}")

    def _enhance_query(self, question: str) -> Dict[str, Any]:
        """ç®€å•çš„æŸ¥è¯¢æ”¹å†™ - ä¸‰æ­¥èµ°"""
        try:
            # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨TopicClassifierç¡®å®šç›®æ ‡æ–‡æ¡£é›†åˆ
            target_collections = self.topic_classifier.classify(question)
            self.logger.info(f"ğŸ¯ é¢„é€‰æ–‡æ¡£é›†åˆ: {target_collections}")

            # ç¬¬äºŒæ­¥ï¼šå°è¯•åŸºäºTOCè¿›è¡ŒæŸ¥è¯¢å¢å¼ºï¼ˆæ·»åŠ ç›®å½•ä¸Šä¸‹æ–‡ï¼‰
            enhanced_query = self._simple_query_rewrite(question, target_collections)

            if enhanced_query and enhanced_query != question:
                # æŸ¥è¯¢å¢å¼ºæˆåŠŸ
                enhanced_info = {
                    'original_query': question,
                    'keywords': [enhanced_query],  # ä½¿ç”¨å¢å¼ºåçš„æŸ¥è¯¢
                    'collections': target_collections,
                    'enhanced': True,
                    'enhanced_query': enhanced_query,
                    'enhancement_type': 'toc_context_enhanced'
                }

                self.logger.info(f"âœ… æŸ¥è¯¢å¢å¼ºæˆåŠŸ")
                self.logger.info(f"   åŸæŸ¥è¯¢: {question}")
                self.logger.info(f"   å¢å¼ºå: {enhanced_query[:100]}..." if len(enhanced_query) > 100 else f"   å¢å¼ºå: {enhanced_query}")

                return enhanced_info
            else:
                # æŸ¥è¯¢å¢å¼ºå¤±è´¥æˆ–è·³è¿‡ï¼Œä½¿ç”¨åŸæŸ¥è¯¢
                self.logger.info("ğŸ”„ æŸ¥è¯¢å¢å¼ºè·³è¿‡ï¼Œä½¿ç”¨åŸæŸ¥è¯¢")
                return {
                    'original_query': question,
                    'keywords': [question],
                    'collections': target_collections,
                    'enhanced': False,
                    'enhancement_type': 'no_enhancement'
                }

        except Exception as e:
            self.logger.warning(f"æŸ¥è¯¢å¢å¼ºå¤±è´¥: {e}")
            return {
                'original_query': question,
                'keywords': [question],
                'collections': self.topic_classifier.classify(question),
                'enhanced': False,
                'enhancement_type': 'error'
            }

    def _enhanced_retrieve(self, search_queries: List[str], collections: List[str]) -> List[Any]:
        """å¢å¼ºæ£€ç´¢æ–¹æ³• - æ”¯æŒé‡æ’å™¨"""
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªæŸ¥è¯¢è¿›è¡Œæ£€ç´¢
        main_query = search_queries[0] if search_queries else ""

        try:
            # æ£€ç´¢æ›´å¤šæ–‡æ¡£ç”¨äºé‡æ’
            results = self.retriever.retrieve(
                query=main_query,
                top_k=self.top_k,  # ç°åœ¨æ˜¯50ä¸ª
                collection_ids=collections
            )
            self.logger.info(f"ğŸ” åˆå§‹æ£€ç´¢å®Œæˆ: æŸ¥è¯¢'{main_query[:50]}...' è¿”å›{len(results)}ä¸ªç»“æœ")

            # åº”ç”¨é‡æ’å™¨
            if self.reranker and self.reranker.is_enabled() and results:
                # è½¬æ¢ä¸ºé‡æ’å™¨éœ€è¦çš„æ ¼å¼
                docs_for_rerank = []
                for result in results:
                    docs_for_rerank.append({
                        'content': result.content,
                        'score': result.score,
                        'metadata': getattr(result, 'metadata', {}),
                        'original_result': result
                    })

                # æ‰§è¡Œé‡æ’
                reranked_docs = self.reranker.rerank(main_query, docs_for_rerank)

                # è½¬æ¢å›åŸå§‹ç»“æœæ ¼å¼
                final_results = []
                for doc in reranked_docs:
                    original_result = doc['original_result']
                    # æ›´æ–°åˆ†æ•°ä¸ºé‡æ’åˆ†æ•°
                    original_result.score = doc.get('rerank_score', original_result.score)
                    final_results.append(original_result)

                self.logger.info(f"ğŸ¯ é‡æ’å®Œæˆ: {len(results)}ä¸ª â†’ {len(final_results)}ä¸ª")
                return final_results
            else:
                # æ²¡æœ‰é‡æ’å™¨ï¼Œè¿”å›åŸå§‹ç»“æœ
                return results

        except Exception as e:
            self.logger.error(f"æ£€ç´¢å¤±è´¥: {e}")
            return []

    def _simple_query_rewrite(self, query: str, target_collections: List[str]) -> str:
        """æŸ¥è¯¢å¢å¼ºå‡½æ•° - æ·»åŠ ç›®å½•ä¸Šä¸‹æ–‡è€Œä¸æ˜¯æ”¹å†™æŸ¥è¯¢"""
        try:
            # ç¬¬ä¸€æ­¥ï¼šæ¥æ”¶é€‰å®šçš„æ–‡æ¡£é›†åˆä½œä¸ºå‚æ•°ï¼ˆå·²å®Œæˆï¼‰

            # ç¬¬äºŒæ­¥ï¼šè¯»å–å¯¹åº”çš„TOC YAMLæ–‡ä»¶
            toc_content = self._load_toc_for_collections(target_collections)
            if not toc_content:
                self.logger.info("ğŸ“‹ ç›®æ ‡é›†åˆæ— å¯¹åº”TOCæ–‡ä»¶ï¼Œè·³è¿‡æŸ¥è¯¢å¢å¼º")
                return query

            # ç¬¬ä¸‰æ­¥ï¼šæå–ç›¸å…³ç›®å½•å†…å®¹å¹¶ä¸åŸæŸ¥è¯¢åˆå¹¶
            relevant_context = self._extract_relevant_toc_context(query, toc_content)
            if relevant_context:
                enhanced_query = f"{query} | ç›¸å…³æ–‡æ¡£ç»“æ„: {relevant_context}"
                self.logger.info(f"âœ… æŸ¥è¯¢å¢å¼ºæˆåŠŸï¼Œæ·»åŠ äº† {len(relevant_context)} å­—ç¬¦çš„ç›®å½•ä¸Šä¸‹æ–‡")
                return enhanced_query
            else:
                self.logger.info("ğŸ“‹ æœªæ‰¾åˆ°ç›¸å…³ç›®å½•å†…å®¹ï¼Œä½¿ç”¨åŸæŸ¥è¯¢")
                return query

        except Exception as e:
            self.logger.warning(f"æŸ¥è¯¢å¢å¼ºè¿‡ç¨‹å‡ºé”™: {e}")
            return query

    def _load_toc_for_collections(self, collections: List[str]) -> str:
        """ä¸ºæŒ‡å®šé›†åˆåŠ è½½TOCå†…å®¹"""
        import yaml
        from pathlib import Path

        toc_dir = Path("data/toc")
        if not toc_dir.exists():
            return ""

        toc_content_parts = []

        for collection in collections:
            # å°è¯•ä¸åŒçš„æ–‡ä»¶åæ ¼å¼
            possible_files = [
                f"{collection}_toc.yaml",
                f"{collection}.yaml",
                f"{collection}_toc",
                f"{collection}"
            ]

            for filename in possible_files:
                toc_file = toc_dir / filename
                if toc_file.exists():
                    try:
                        with open(toc_file, 'r', encoding='utf-8') as f:
                            toc_data = yaml.safe_load(f)

                        # æ„å»ºç®€æ´çš„TOCå†…å®¹
                        content = self._format_toc_content(collection, toc_data)
                        if content:
                            toc_content_parts.append(content)
                            self.logger.info(f"âœ… åŠ è½½TOCæ–‡ä»¶: {filename}")
                        break
                    except Exception as e:
                        self.logger.warning(f"è¯»å–TOCæ–‡ä»¶å¤±è´¥ {filename}: {e}")

        return "\n\n".join(toc_content_parts)

    def _format_toc_content(self, collection: str, toc_data: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–TOCå†…å®¹ä¸ºç®€æ´çš„æ–‡æœ¬"""
        if not toc_data:
            return ""

        lines = [f"=== {collection} ç›®å½•ç»“æ„ ==="]

        chapters = toc_data.get('chapters', [])
        for chapter in chapters:
            chapter_title = chapter.get('title', '')
            chapter_num = chapter.get('chapter_num', '')

            if chapter_title:
                lines.append(f"{chapter_num} {chapter_title}")

                # æ·»åŠ å­ç« èŠ‚
                subsections = chapter.get('subsections', [])
                for subsection in subsections:
                    subsection_title = subsection.get('title', '')
                    if subsection_title:
                        lines.append(f"  - {subsection_title}")

        return "\n".join(lines)

    def _extract_relevant_toc_context(self, query: str, toc_content: str) -> str:
        """ä½¿ç”¨LLMä»TOCå†…å®¹ä¸­æå–ä¸æŸ¥è¯¢ç›¸å…³çš„ç›®å½•ä¸Šä¸‹æ–‡"""
        try:
            prompt = f"""è¯·åˆ†æç”¨æˆ·æŸ¥è¯¢ï¼Œä»ä»¥ä¸‹æ–‡æ¡£ç›®å½•ç»“æ„ä¸­æå–ä¸æŸ¥è¯¢æœ€ç›¸å…³çš„ç« èŠ‚å’Œå­ç« èŠ‚ä¿¡æ¯ã€‚

ç”¨æˆ·æŸ¥è¯¢: {query}

æ–‡æ¡£ç›®å½•ç»“æ„:
{toc_content}

è¯·æ‰¾å‡ºä¸ç”¨æˆ·æŸ¥è¯¢æœ€ç›¸å…³çš„ç« èŠ‚å’Œå­ç« èŠ‚ï¼Œå¹¶æŒ‰ä»¥ä¸‹æ ¼å¼è¿”å›ï¼š
- åªè¿”å›ç›¸å…³çš„ç« èŠ‚æ ‡é¢˜å’Œå­ç« èŠ‚åç§°
- ç”¨ç®€æ´çš„æ ¼å¼ç»„ç»‡ä¿¡æ¯
- å¦‚æœæ²¡æœ‰ç›¸å…³å†…å®¹ï¼Œè¿”å›"æ— ç›¸å…³å†…å®¹"

è¯·ç›´æ¥è¿”å›ç›¸å…³çš„ç›®å½•ä¿¡æ¯ï¼Œä¸è¦å…¶ä»–è§£é‡Šï¼š"""

            response = self.llm.generate(prompt, max_tokens=300, temperature=0.1)
            relevant_context = response.strip()

            # éªŒè¯å“åº”
            if relevant_context and relevant_context != "æ— ç›¸å…³å†…å®¹" and len(relevant_context) > 10:
                return relevant_context
            else:
                return ""

        except Exception as e:
            self.logger.warning(f"LLMæå–ç›¸å…³TOCä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return ""

    def _llm_rewrite_query(self, query: str, toc_content: str) -> str:
        """ä½¿ç”¨LLMæ ¹æ®TOCå†…å®¹æ”¹å†™æŸ¥è¯¢"""
        prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹æ–‡æ¡£ç›®å½•ç»“æ„ï¼Œæ”¹å†™ç”¨æˆ·æŸ¥è¯¢ä»¥æé«˜æœç´¢å‡†ç¡®æ€§ã€‚

åŸå§‹æŸ¥è¯¢: {query}

æ–‡æ¡£ç›®å½•ç»“æ„:
{toc_content}

è¯·åˆ†æç”¨æˆ·æŸ¥è¯¢ä¸ç›®å½•ç»“æ„çš„å…³è”ï¼Œç„¶åç”Ÿæˆä¸€ä¸ªæ›´ç²¾ç¡®çš„æŸ¥è¯¢ã€‚æ”¹å†™è¦æ±‚ï¼š
1. ä¿æŒæŸ¥è¯¢çš„æ ¸å¿ƒæ„å›¾ä¸å˜
2. åŠ å…¥ç›®å½•ä¸­ç›¸å…³çš„å…·ä½“æœ¯è¯­å’Œè¡¨æ ¼åç§°
3. ä½¿æŸ¥è¯¢æ›´å®¹æ˜“åŒ¹é…åˆ°ç›¸å…³æ–‡æ¡£å†…å®¹
4. å¦‚æœæ‰¾ä¸åˆ°ç›¸å…³å†…å®¹ï¼Œè¿”å›åŸæŸ¥è¯¢

è¯·ç›´æ¥è¿”å›æ”¹å†™åçš„æŸ¥è¯¢ï¼Œä¸è¦å…¶ä»–è§£é‡Šï¼š"""

        try:
            response = self.llm.generate(prompt, max_tokens=200, temperature=0.1)
            rewritten = response.strip()

            # ç®€å•éªŒè¯ï¼šæ”¹å†™åçš„æŸ¥è¯¢ä¸èƒ½ä¸ºç©ºä¸”ä¸èƒ½è¿‡é•¿
            if rewritten and len(rewritten) > 5 and len(rewritten) < 200:
                return rewritten
            else:
                return query

        except Exception as e:
            self.logger.warning(f"LLMæŸ¥è¯¢æ”¹å†™å¤±è´¥: {e}")
            return query

    def get_system_status(self) -> Dict[str, Any]:
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        return {
            'system_type': 'UnifiedRAGSystem',
            'configuration': {
                'top_k': self.top_k,
                'similarity_threshold': self.similarity_threshold
            },
            'components': {
                'llm_available': hasattr(self, 'llm') and self.llm is not None,
                'retriever_available': hasattr(self, 'retriever') and self.retriever is not None,
                'topic_classifier_available': hasattr(self, 'topic_classifier') and self.topic_classifier is not None,
                'toc_enhancement_available': True,  # TOCå¢å¼ºåŠŸèƒ½æ€»æ˜¯å¯ç”¨
                'reranker_available': hasattr(self, 'reranker') and self.reranker is not None,
                'reranker_enabled': hasattr(self, 'reranker') and self.reranker and self.reranker.is_enabled()
            },
            'collections': {
                'configured': len(self.config_manager.get('retrieval.collections', [])),
                'loaded': len(self.retriever.collections) if hasattr(self, 'retriever') else 0
            }
        }
