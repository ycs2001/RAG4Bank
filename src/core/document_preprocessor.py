"""
æ–‡æ¡£é¢„å¤„ç†å™¨ï¼šåŸºäºGROBID DockeræœåŠ¡ + LLMçš„æ™ºèƒ½ç›®å½•æå–
"""

import requests
import json
import yaml
import subprocess
import tempfile
from datetime import datetime
from typing import List, Dict, Any, Optional
from pathlib import Path
from .base_component import BaseComponent
from ..config import EnhancedConfigManager

class DocumentPreprocessor(BaseComponent):
    """æ–‡æ¡£é¢„å¤„ç†å™¨ - ä½¿ç”¨gorbid + LLMæ™ºèƒ½æå–ç›®å½•"""

    def __init__(self, config_manager: EnhancedConfigManager, llm=None):
        """
        åˆå§‹åŒ–æ–‡æ¡£é¢„å¤„ç†å™¨

        Args:
            config_manager: é…ç½®ç®¡ç†å™¨å®ä¾‹
            llm: LLMå®ä¾‹ï¼Œç”¨äºæ™ºèƒ½åˆ†æ
        """
        super().__init__(config_manager)

        self.llm = llm
        self.preprocessing_config = self.get_config('documents.preprocessing', {})
        self.enabled = self.preprocessing_config.get('enabled', True)
        self.ocr_pages_limit = self.preprocessing_config.get('ocr_pages_limit', 10)
        self.toc_extraction_enabled = self.preprocessing_config.get('toc_extraction_enabled', True)

        # GROBIDæœåŠ¡é…ç½®
        self.grobid_url = self.preprocessing_config.get('grobid_url', 'http://localhost:8070')
        self.grobid_timeout = self.preprocessing_config.get('grobid_timeout', 300)

        if self.enabled:
            self._check_dependencies()
            self.logger.info("âœ… æ–‡æ¡£é¢„å¤„ç†å™¨å·²å¯ç”¨ (GROBID Docker + LLM)")
        else:
            self.logger.info("â„¹ï¸ æ–‡æ¡£é¢„å¤„ç†å™¨å·²ç¦ç”¨")
    
    def _check_dependencies(self):
        """æ£€æŸ¥GROBIDæœåŠ¡å’ŒLLMæ˜¯å¦å¯ç”¨"""
        try:
            # å°è¯•å¤šä¸ªå¯èƒ½çš„GROBIDç«¯å£
            possible_urls = [
                "http://localhost:8070",
                "http://localhost:8080",
                "http://localhost:8071"
            ]

            grobid_available = False
            for url in possible_urls:
                try:
                    health_url = f"{url}/api/isalive"
                    response = requests.get(health_url, timeout=5)

                    if response.status_code == 200:
                        self.grobid_url = url  # æ›´æ–°ä¸ºå¯ç”¨çš„URL
                        self.logger.info(f"âœ… GROBIDæœåŠ¡è¿æ¥æˆåŠŸ: {url}")

                        # è·å–GROBIDç‰ˆæœ¬ä¿¡æ¯
                        try:
                            version_url = f"{url}/api/version"
                            version_response = requests.get(version_url, timeout=3)
                            if version_response.status_code == 200:
                                self.logger.info(f"ğŸ“‹ GROBIDç‰ˆæœ¬: {version_response.text.strip()}")
                        except:
                            pass

                        grobid_available = True
                        break
                except:
                    continue

            if not grobid_available:
                self.logger.warning("âš ï¸ GROBIDæœåŠ¡åœ¨æ‰€æœ‰ç«¯å£éƒ½ä¸å¯ç”¨ï¼Œå°†è·³è¿‡GROBIDæ£€æŸ¥ç»§ç»­è¿è¡Œ")
                # ä¸ç¦ç”¨æœåŠ¡ï¼Œå…è®¸ç»§ç»­è¿è¡Œ

            # æ£€æŸ¥LLMæ˜¯å¦å¯ç”¨
            if not self.llm:
                self.logger.warning("âš ï¸ LLMæœªæä¾›ï¼Œç›®å½•åˆ†æåŠŸèƒ½å°†å—é™")

            self.logger.info("âœ… ä¾èµ–æ£€æŸ¥å®Œæˆ")

        except Exception as e:
            self.logger.error(f"ä¾èµ–æ£€æŸ¥å¤±è´¥: {e}")
            # ä¸ç¦ç”¨æœåŠ¡ï¼Œå…è®¸ç»§ç»­è¿è¡Œ
    
    def extract_document_toc(self, document_id: str, file_path: str) -> Dict[str, Any]:
        """
        ä½¿ç”¨gorbid + LLMæå–æ–‡æ¡£ç›®å½•ç»“æ„

        Args:
            document_id: æ–‡æ¡£ID
            file_path: æ–‡æ¡£æ–‡ä»¶è·¯å¾„

        Returns:
            ç›®å½•ç»“æ„ä¿¡æ¯
        """
        if not self.enabled or not self.toc_extraction_enabled:
            return {'status': 'disabled', 'chapters': []}

        try:
            self.logger.info(f"å¼€å§‹æå–æ–‡æ¡£ç›®å½•: {document_id}")

            file_path = Path(file_path)
            if not file_path.exists():
                self.logger.error(f"æ–‡æ¡£æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return {'status': 'file_not_found', 'chapters': []}

            # æ­¥éª¤1ï¼šæ ¹æ®æ–‡ä»¶ç±»å‹æå–æ–‡æœ¬
            if file_path.suffix.lower() == '.pdf':
                extracted_text = self._extract_text_with_grobid(file_path)
            elif file_path.suffix.lower() in ['.docx', '.doc']:
                # ä½¿ç”¨LibreOfficeè½¬æ¢ä¸ºPDFï¼Œç„¶åç”¨GROBIDæå–
                pdf_path = self._convert_docx_to_pdf(file_path)
                if pdf_path:
                    extracted_text = self._extract_text_with_grobid(pdf_path)
                    # æ¸…ç†ä¸´æ—¶PDFæ–‡ä»¶
                    try:
                        pdf_path.unlink()
                    except:
                        pass
                else:
                    extracted_text = None
            elif file_path.suffix.lower() in ['.xlsx', '.xls']:
                self.logger.warning(f"è·³è¿‡Excelæ–‡ä»¶çš„ç›®å½•æå–: {file_path}")
                return {'status': 'unsupported_format', 'chapters': []}
            else:
                # å°è¯•ç›´æ¥è¯»å–æ–‡æœ¬æ–‡ä»¶
                extracted_text = self._extract_text_from_file(file_path)

            if not extracted_text:
                return {'status': 'text_extraction_failed', 'chapters': []}

            # æ­¥éª¤2ï¼šä½¿ç”¨LLMåˆ†æç›®å½•ç»“æ„
            toc_data = self._analyze_toc_with_llm(extracted_text, document_id)

            # æ­¥éª¤3ï¼šæ›´æ–°é…ç½®æ–‡ä»¶
            self._update_toc_config(document_id, toc_data)

            return toc_data

        except Exception as e:
            self.logger.error(f"æå–æ–‡æ¡£ç›®å½•å¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e), 'chapters': []}
    
    def _extract_text_with_grobid(self, file_path: Path) -> Optional[str]:
        """ä½¿ç”¨GROBID DockeræœåŠ¡æå–æ–‡æ¡£æ–‡æœ¬"""
        try:
            self.logger.info(f"ä½¿ç”¨GROBIDæå–æ–‡æœ¬: {file_path}")

            # ä½¿ç”¨processFulltextDocument APIè·å–æ›´å®Œæ•´çš„æ–‡æ¡£ç»“æ„
            api_url = f"{self.grobid_url}/api/processFulltextDocument"

            # è®¾ç½®è¯·æ±‚å‚æ•° - å¤„ç†æ›´å¤šé¡µé¢ä»¥è·å–å®Œæ•´ç›®å½•
            params = {
                'start': 1,
                'end': min(self.ocr_pages_limit * 2, 20),  # æ‰©å¤§é¡µé¢èŒƒå›´
                'consolidateHeader': 0,
                'consolidateCitations': 0,
                'includeRawCitations': 0
            }

            # å‡†å¤‡æ–‡ä»¶
            with open(file_path, 'rb') as pdf_file:
                files = {'input': pdf_file}
                headers = {'Accept': 'application/xml'}

                # å‘é€è¯·æ±‚
                response = requests.post(
                    api_url,
                    files=files,
                    data=params,
                    headers=headers,
                    timeout=self.grobid_timeout
                )

            if response.status_code == 200:
                # ä»TEI XMLä¸­æå–æ–‡æœ¬å†…å®¹
                text_content = self._extract_text_from_tei(response.text)

                if not text_content:
                    self.logger.warning("GROBIDæå–çš„æ–‡æœ¬ä¸ºç©º")
                    return None

                self.logger.info(f"GROBIDæå–æˆåŠŸï¼Œæ–‡æœ¬é•¿åº¦: {len(text_content)} å­—ç¬¦")
                return text_content

            elif response.status_code == 204:
                self.logger.warning("GROBIDå¤„ç†å®Œæˆï¼Œä½†æœªæå–åˆ°å†…å®¹")
                return None
            elif response.status_code == 503:
                self.logger.error("GROBIDæœåŠ¡ç¹å¿™ï¼Œè¯·ç¨åé‡è¯•")
                return None
            else:
                self.logger.error(f"GROBIDå¤„ç†å¤±è´¥: HTTP {response.status_code}")
                return None

        except requests.exceptions.Timeout:
            self.logger.error("GROBIDå¤„ç†è¶…æ—¶")
            return None
        except requests.exceptions.RequestException as e:
            self.logger.error(f"GROBIDè¯·æ±‚å¤±è´¥: {e}")
            return None
        except Exception as e:
            self.logger.error(f"GROBIDæ–‡æœ¬æå–å¤±è´¥: {e}")
            return None

    def _extract_text_from_tei(self, tei_xml: str) -> str:
        """ä»TEI XMLä¸­æå–çº¯æ–‡æœ¬å†…å®¹"""
        try:
            import xml.etree.ElementTree as ET

            # è§£æXML
            root = ET.fromstring(tei_xml)

            # æå–æ‰€æœ‰æ–‡æœ¬å†…å®¹
            text_parts = []

            # éå†æ‰€æœ‰æ–‡æœ¬èŠ‚ç‚¹
            for elem in root.iter():
                if elem.text:
                    text_parts.append(elem.text.strip())
                if elem.tail:
                    text_parts.append(elem.tail.strip())

            # åˆå¹¶æ–‡æœ¬å¹¶æ¸…ç†
            full_text = ' '.join(text_parts)

            # æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
            import re
            full_text = re.sub(r'\s+', ' ', full_text).strip()

            return full_text

        except ET.ParseError as e:
            self.logger.error(f"TEI XMLè§£æå¤±è´¥: {e}")
            # å¦‚æœXMLè§£æå¤±è´¥ï¼Œå°è¯•ç›´æ¥è¿”å›æ–‡æœ¬å†…å®¹
            return tei_xml
        except Exception as e:
            self.logger.error(f"ä»TEIæå–æ–‡æœ¬å¤±è´¥: {e}")
            return tei_xml

    def _convert_docx_to_pdf(self, docx_path: Path) -> Optional[Path]:
        """ä½¿ç”¨LibreOfficeå°†DOCXè½¬æ¢ä¸ºPDF"""
        try:
            self.logger.info(f"ä½¿ç”¨LibreOfficeè½¬æ¢DOCXä¸ºPDF: {docx_path}")

            # åˆ›å»ºä¸´æ—¶ç›®å½•
            temp_dir = Path(tempfile.mkdtemp())

            # LibreOfficeè½¬æ¢å‘½ä»¤ - å°è¯•å¤šä¸ªå¯èƒ½çš„è·¯å¾„
            possible_commands = [
                '/Applications/LibreOffice.app/Contents/MacOS/soffice',
                'soffice',
                'libreoffice'
            ]

            cmd = None
            for soffice_cmd in possible_commands:
                try:
                    # æµ‹è¯•å‘½ä»¤æ˜¯å¦å¯ç”¨
                    test_result = subprocess.run([soffice_cmd, '--version'],
                                                capture_output=True, timeout=10)
                    if test_result.returncode == 0:
                        cmd = [
                            soffice_cmd,
                            '--headless',
                            '--convert-to', 'pdf',
                            '--outdir', str(temp_dir),
                            str(docx_path)
                        ]
                        break
                except:
                    continue

            if not cmd:
                self.logger.error("LibreOffice/sofficeæœªæ‰¾åˆ°")
                return None

            # æ‰§è¡Œè½¬æ¢
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)

            if result.returncode == 0:
                # æŸ¥æ‰¾ç”Ÿæˆçš„PDFæ–‡ä»¶
                pdf_name = docx_path.stem + '.pdf'
                pdf_path = temp_dir / pdf_name

                if pdf_path.exists():
                    self.logger.info(f"DOCXè½¬PDFæˆåŠŸ: {pdf_path}")
                    return pdf_path
                else:
                    self.logger.error("PDFæ–‡ä»¶æœªç”Ÿæˆ")
                    return None
            else:
                self.logger.error(f"LibreOfficeè½¬æ¢å¤±è´¥: {result.stderr}")
                return None

        except subprocess.TimeoutExpired:
            self.logger.error("LibreOfficeè½¬æ¢è¶…æ—¶")
            return None
        except FileNotFoundError:
            self.logger.error("LibreOfficeæœªå®‰è£…æˆ–ä¸åœ¨PATHä¸­")
            self.logger.error("è¯·å®‰è£…LibreOffice: brew install --cask libreoffice")
            return None
        except Exception as e:
            self.logger.error(f"DOCXè½¬PDFå¤±è´¥: {e}")
            return None



    def _extract_text_from_file(self, file_path: Path) -> Optional[str]:
        """ä»æ™®é€šæ–‡æœ¬æ–‡ä»¶æå–æ–‡æœ¬"""
        try:
            self.logger.info(f"ä»æ–‡æœ¬æ–‡ä»¶æå–æ–‡æœ¬: {file_path}")

            with open(file_path, 'r', encoding='utf-8') as f:
                text = f.read()

            self.logger.info(f"æ–‡æœ¬æ–‡ä»¶æå–æˆåŠŸï¼Œæ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
            return text

        except UnicodeDecodeError:
            # å°è¯•å…¶ä»–ç¼–ç 
            try:
                with open(file_path, 'r', encoding='gbk') as f:
                    text = f.read()
                self.logger.info(f"æ–‡æœ¬æ–‡ä»¶æå–æˆåŠŸï¼ˆGBKç¼–ç ï¼‰ï¼Œæ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
                return text
            except Exception as e:
                self.logger.error(f"æ–‡æœ¬æ–‡ä»¶æå–å¤±è´¥: {e}")
                return None
        except Exception as e:
            self.logger.error(f"æ–‡æœ¬æ–‡ä»¶æå–å¤±è´¥: {e}")
            return None
    
    def _analyze_toc_with_llm(self, text_content: str, document_id: str) -> Dict[str, Any]:
        """ä½¿ç”¨LLMåˆ†ææ–‡æ¡£ç›®å½•ç»“æ„"""
        if not self.llm:
            self.logger.warning("LLMæœªæä¾›ï¼Œæ— æ³•è¿›è¡Œæ™ºèƒ½ç›®å½•åˆ†æ")
            return {'status': 'llm_unavailable', 'chapters': []}

        try:
            self.logger.info(f"ä½¿ç”¨LLMåˆ†ææ–‡æ¡£ç›®å½•: {document_id}")

            # è·å–ç›®å½•åˆ†æçš„promptæ¨¡æ¿
            prompt_template = self.get_config('documents.toc_analysis_prompt', self._get_default_toc_prompt())

            # æ„å»ºåˆ†æprompt - æ‰©å¤§æ–‡æœ¬åˆ†æèŒƒå›´
            max_text_length = 20000  # å¢åŠ åˆ°20000å­—ç¬¦ä»¥è·å–å®Œæ•´ç›®å½•
            prompt = prompt_template.format(
                document_id=document_id,
                text_content=text_content[:max_text_length],
                max_pages=self.ocr_pages_limit
            )

            # è°ƒç”¨LLMåˆ†æ - å¢åŠ max_tokensä»¥è·å–å®Œæ•´å“åº”
            messages = [{"role": "user", "content": prompt}]
            response = self.llm.chat(messages, temperature=0.1, max_tokens=8000)

            # è§£æLLMå“åº”
            toc_data = self._parse_llm_toc_response(response.text, document_id)

            return toc_data

        except Exception as e:
            self.logger.error(f"LLMç›®å½•åˆ†æå¤±è´¥: {e}")
            return {'status': 'llm_analysis_failed', 'error': str(e), 'chapters': []}
    

    
    def _update_toc_config(self, document_id: str, toc_data: Dict[str, Any]):
        """æ›´æ–°é…ç½®æ–‡ä»¶ä¸­çš„ç›®å½•ä¿¡æ¯"""
        try:
            # ä¿å­˜åˆ°å•ç‹¬çš„ç›®å½•æ•°æ®æ–‡ä»¶ï¼ˆä½¿ç”¨YAMLæ ¼å¼ï¼‰
            toc_file_path = Path('data/toc') / f'{document_id}_toc.yaml'
            toc_file_path.parent.mkdir(parents=True, exist_ok=True)

            # ä¿å­˜è¯¦ç»†çš„ç›®å½•æ•°æ®
            with open(toc_file_path, 'w', encoding='utf-8') as f:
                yaml.dump(toc_data, f, default_flow_style=False, allow_unicode=True, indent=2)

            self.logger.info(f"âœ… ç›®å½•æ•°æ®å·²ä¿å­˜: {toc_file_path}")
            self.logger.info(f"ğŸ“‹ æ–‡æ¡£ {document_id} æå–å®Œæˆï¼Œå…± {len(toc_data.get('chapters', []))} ä¸ªç« èŠ‚")

            # å°è¯•æ›´æ–°é…ç½®çŠ¶æ€ï¼ˆå¦‚æœæ”¯æŒçš„è¯ï¼‰
            self._try_update_config_status(document_id, toc_data)

        except Exception as e:
            self.logger.error(f"æ›´æ–°ç›®å½•é…ç½®å¤±è´¥: {e}")

    def _try_update_config_status(self, document_id: str, toc_data: Dict[str, Any]):
        """å°è¯•æ›´æ–°é…ç½®æ–‡ä»¶ä¸­çš„çŠ¶æ€ä¿¡æ¯"""
        try:
            # è¿™é‡Œå¯ä»¥å®ç°é…ç½®æ–‡ä»¶çš„çŠ¶æ€æ›´æ–°
            # ç›®å‰å…ˆè®°å½•åˆ°æ—¥å¿—ï¼Œåç»­å¯ä»¥æ‰©å±•ä¸ºå®é™…çš„é…ç½®æ–‡ä»¶æ›´æ–°
            status = toc_data.get('status', 'unknown')
            confidence = toc_data.get('confidence', 0.0)
            chapters_count = len(toc_data.get('chapters', []))

            self.logger.info(f"ğŸ“Š {document_id} çŠ¶æ€æ›´æ–°:")
            self.logger.info(f"   - æå–çŠ¶æ€: {status}")
            self.logger.info(f"   - ç½®ä¿¡åº¦: {confidence:.2f}")
            self.logger.info(f"   - ç« èŠ‚æ•°: {chapters_count}")

        except Exception as e:
            self.logger.error(f"æ›´æ–°é…ç½®çŠ¶æ€å¤±è´¥: {e}")

    def load_document_toc(self, document_id: str) -> Optional[Dict[str, Any]]:
        """åŠ è½½æ–‡æ¡£çš„ç›®å½•æ•°æ®"""
        try:
            toc_file_path = Path('data/toc') / f'{document_id}_toc.yaml'

            if not toc_file_path.exists():
                return None

            with open(toc_file_path, 'r', encoding='utf-8') as f:
                toc_data = yaml.safe_load(f)

            return toc_data

        except Exception as e:
            self.logger.error(f"åŠ è½½ç›®å½•æ•°æ®å¤±è´¥: {e}")
            return None
    
    def process_all_documents(self) -> Dict[str, Any]:
        """å¤„ç†æ‰€æœ‰é…ç½®çš„æ–‡æ¡£"""
        if not self.enabled:
            return {'status': 'disabled'}
        
        toc_config = self.get_config('documents.toc', {})
        results = {}
        
        for document_id, doc_info in toc_config.items():
            if doc_info.get('extraction_status') == 'pending':
                file_path = doc_info.get('file_path')
                if file_path:
                    result = self.extract_document_toc(document_id, file_path)
                    results[document_id] = result
        
        return {
            'status': 'completed',
            'processed_documents': len(results),
            'results': results
        }

    def _get_default_toc_prompt(self) -> str:
        """è·å–é»˜è®¤çš„ç›®å½•åˆ†æpromptæ¨¡æ¿"""
        return """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£ç›®å½•åˆ†æä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹æ–‡æ¡£æ–‡æœ¬ï¼Œæå–å…¶ç›®å½•ç»“æ„ã€‚

æ–‡æ¡£ID: {document_id}
æ–‡æ¡£æ–‡æœ¬:
{text_content}

è¯·ä»”ç»†åˆ†ææ–‡æœ¬å†…å®¹ï¼Œå¯»æ‰¾ä»¥ä¸‹ç›®å½•ç‰¹å¾ï¼š

**ä¸»è¦ç›®å½•æ¨¡å¼ï¼š**
1. æŠ¥è¡¨æ±‡æ€»è¡¨æ ¼å¼ï¼šåŒ…å«"åºå·"ã€"è¡¨å·"ã€"æŠ¥è¡¨åç§°"ç­‰åˆ—
2. ç« èŠ‚ç¼–å·ï¼šå¦‚"ç¬¬ä¸€ç« "ã€"ä¸€ã€"ã€"äºŒã€"ç­‰
3. è¡¨æ ¼ç¼–å·ï¼šå¦‚"G01"ã€"G03"ã€"G4A"ã€"S63"ç­‰
4. é¡µç ä¿¡æ¯ï¼šæ•°å­—é¡µç æˆ–"æœˆ"ã€"å­£"ã€"åŠå¹´"ç­‰é¢‘åº¦ä¿¡æ¯

**é“¶è¡Œç›‘ç®¡æŠ¥è¡¨ç‰¹å¾ï¼š**
- åŸºç¡€è´¢åŠ¡ç±»ï¼šG01ã€G03ã€G04ã€G05ç­‰
- èµ„æœ¬å……è¶³ç±»ï¼šG40ã€G4Aã€G4Bã€G4Cã€G4Dç­‰
- é£é™©ç®¡ç†ç±»ï¼šG11ã€G12ã€G13ã€G14ç­‰
- ä¸“é¡¹ç»Ÿè®¡ç±»ï¼šS63ã€S68ã€S70ç­‰

**å±‚çº§å…³ç³»è¯†åˆ«ï¼š**
- ä¸»åˆ†ç±»ï¼šåŸºç¡€è´¢åŠ¡ã€è‚¡ä¸œæƒ…å†µã€æ æ†ç‡ã€èµ„æœ¬å……è¶³ã€ä¿¡ç”¨é£é™©ç­‰
- å­è¡¨æ ¼ï¼šæ¯ä¸ªä¸»åˆ†ç±»ä¸‹çš„å…·ä½“æŠ¥è¡¨
- é™„æ³¨éƒ¨åˆ†ï¼šè¡¨æ ¼çš„è¯¦ç»†è¯´æ˜å’Œå¡«æŠ¥è¦æ±‚

è¯·æå–æ‰€æœ‰æ‰¾åˆ°çš„è¡¨æ ¼å’Œç« èŠ‚ä¿¡æ¯ï¼Œä¸è¦é—æ¼ä»»ä½•æŠ¥è¡¨ã€‚

è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç›®å½•ç»“æ„ï¼š
{{
    "status": "completed",
    "extraction_method": "llm_analysis",
    "last_updated": "å½“å‰æ—¶é—´",
    "confidence": 0.85,
    "chapters": [
        {{
            "chapter_num": "ç¬¬ä¸€ç« ",
            "title": "ç« èŠ‚æ ‡é¢˜",
            "page": 1,
            "level": 1,
            "subsections": [
                {{
                    "chapter_num": "1.1",
                    "title": "å­ç« èŠ‚æ ‡é¢˜",
                    "page": 5,
                    "level": 2
                }}
            ]
        }}
    ]
}}

æ³¨æ„äº‹é¡¹ï¼š
1. å¦‚æœæ–‡æœ¬ä¸­æ²¡æœ‰æ˜æ˜¾çš„ç›®å½•ç»“æ„ï¼Œè¿”å›ç©ºçš„chaptersæ•°ç»„
2. å°½é‡å‡†ç¡®è¯†åˆ«ç« èŠ‚çš„å±‚çº§å…³ç³»
3. å¦‚æœé¡µç ä¿¡æ¯ä¸æ˜ç¡®ï¼Œå¯ä»¥è®¾ç½®ä¸ºnull
4. confidenceè¡¨ç¤ºè¯†åˆ«çš„ç½®ä¿¡åº¦ï¼ˆ0.0-1.0ï¼‰
5. åªè¿”å›JSONæ ¼å¼ï¼Œä¸è¦æ·»åŠ å…¶ä»–è¯´æ˜æ–‡å­—"""

    def _parse_llm_toc_response(self, response_text: str, document_id: str) -> Dict[str, Any]:
        """è§£æLLMçš„ç›®å½•åˆ†æå“åº”"""
        try:
            # è®°å½•åŸå§‹å“åº”ç”¨äºè°ƒè¯•
            self.logger.debug(f"LLMåŸå§‹å“åº”é•¿åº¦: {len(response_text)}")
            self.logger.debug(f"LLMå“åº”å‰500å­—ç¬¦: {response_text[:500]}")

            # å°è¯•æå–JSONéƒ¨åˆ†
            response_text = response_text.strip()

            # å¦‚æœå“åº”è¢«åŒ…è£…åœ¨ä»£ç å—ä¸­ï¼Œæå–JSONéƒ¨åˆ†
            if '```json' in response_text:
                start = response_text.find('```json') + 7
                end = response_text.find('```', start)
                if end > start:
                    response_text = response_text[start:end].strip()
            elif '```' in response_text:
                start = response_text.find('```') + 3
                end = response_text.find('```', start)
                if end > start:
                    response_text = response_text[start:end].strip()

            # å°è¯•è§£æJSONå“åº”
            toc_data = json.loads(response_text)

            # éªŒè¯å“åº”æ ¼å¼
            if 'chapters' not in toc_data:
                self.logger.warning("LLMå“åº”ç¼ºå°‘chapterså­—æ®µ")
                toc_data['chapters'] = []

            # æ·»åŠ æ—¶é—´æˆ³
            toc_data['last_updated'] = datetime.now().isoformat()
            toc_data['document_id'] = document_id

            chapters_count = len(toc_data.get('chapters', []))
            confidence = toc_data.get('confidence', 0.0)

            self.logger.info(f"LLMç›®å½•åˆ†æå®Œæˆ: {document_id}, {chapters_count}ä¸ªç« èŠ‚, ç½®ä¿¡åº¦: {confidence}")

            return toc_data

        except json.JSONDecodeError:
            self.logger.error("LLMå“åº”ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼")
            # å°è¯•ä»æ–‡æœ¬ä¸­æå–åŸºæœ¬ä¿¡æ¯
            return self._fallback_parse_toc_response(response_text, document_id)
        except Exception as e:
            self.logger.error(f"è§£æLLMç›®å½•å“åº”å¤±è´¥: {e}")
            return {
                'status': 'parse_failed',
                'error': str(e),
                'document_id': document_id,
                'last_updated': datetime.now().isoformat(),
                'chapters': []
            }

    def _fallback_parse_toc_response(self, response_text: str, document_id: str) -> Dict[str, Any]:
        """LLMå“åº”è§£æå¤±è´¥æ—¶çš„å…œåº•æ–¹æ³•"""
        self.logger.info("ä½¿ç”¨å…œåº•æ–¹æ³•è§£æLLMå“åº”")

        # ç®€å•çš„æ–‡æœ¬è§£æï¼Œæå–å¯èƒ½çš„ç« èŠ‚ä¿¡æ¯
        lines = response_text.split('\n')
        chapters = []

        for line in lines:
            line = line.strip()
            if 'ç« ' in line or 'èŠ‚' in line or line.startswith(('ç¬¬', 'ä¸€', 'äºŒ', 'ä¸‰', 'å››', 'äº”')):
                chapters.append({
                    'title': line,
                    'page': None,
                    'level': 1,
                    'subsections': []
                })

        return {
            'status': 'fallback_parsed',
            'extraction_method': 'llm_fallback',
            'document_id': document_id,
            'last_updated': datetime.now().isoformat(),
            'confidence': 0.3,
            'chapters': chapters
        }
