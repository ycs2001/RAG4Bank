#!/usr/bin/env python3
"""
CategoryRAGå¤šé›†åˆæ•°æ®åº“æ„å»ºå™¨
è´Ÿè´£å°†æ–‡æ¡£åˆ†å—æŒ‰ä¸»é¢˜åˆ†ç±»å­˜å‚¨åˆ°å¯¹åº”çš„ChromaDBé›†åˆä¸­
"""

import os
import sys
import logging
from pathlib import Path
from typing import Dict, List, Any

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from src.config.enhanced_config_manager import EnhancedConfigManager
from src.retrievers import ChromaDBRetriever

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class CollectionDatabaseBuilder:
    """CategoryRAGå¤šé›†åˆæ•°æ®åº“æ„å»ºå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ„å»ºå™¨"""
        self.config_manager = EnhancedConfigManager()
        self.retriever = None
        
        # æ–‡æ¡£åˆ°é›†åˆçš„æ˜ å°„è§„åˆ™
        self.doc_to_collection_mapping = {
            'äººæ°‘é“¶è¡Œé‡‘èç»Ÿè®¡åˆ¶åº¦æ±‡ç¼–': 'pboc_statistics',
            '1104æŠ¥è¡¨åˆè¾‘ã€2024ç‰ˆã€‘': 'report_1104_2024',
            '1104æŠ¥è¡¨åˆè¾‘ã€2022ç‰ˆã€‘': 'report_1104_2022',
            'EASTæ•°æ®ç»“æ„': 'east_data_structure',
            'EASTå…ƒæ•°æ®è¯´æ˜': 'east_metadata',
            'EASTè‡ªè¥èµ„é‡‘æŠ¥é€èŒƒå›´': 'east_data_structure',
            'EASTè¡¨ç»“æ„': 'east_data_structure',
            'ä¸€è¡¨é€šæ•°æ®ç»“æ„': 'ybt_data_structure',
            'ä¸€è¡¨é€šäº§å“æŠ¥é€æ˜ å°„': 'ybt_product_mapping',
            'XXé“¶è¡Œé‘«æ‚¦ç»“æ„æ€§å­˜æ¬¾äº§å“ç®¡ç†åŠæ³•ï¼ˆè¯•è¡Œï¼‰': 'bank_product_management',
            'ç™½çš®ä¹¦å‚è€ƒ': 'regulatory_reference',
            'ç›‘ç®¡å£å¾„ç­”ç–‘æ–‡æ¡£_v1.0': 'regulatory_qa_guidance'
        }
        
    def build(self):
        """é‡æ–°æ„å»ºå¤šé›†åˆæ•°æ®åº“"""
        logger.info("ğŸš€ å¼€å§‹æ„å»ºå¤šé›†åˆæ•°æ®åº“")
        
        try:
            # 1. åˆå§‹åŒ–æ£€ç´¢å™¨
            self._init_retriever()
            
            # 2. å¤„ç†æ¯ä¸ªæ–‡æ¡£ç±»å‹
            chunks_dir = Path("data/processed_docs/chunks")
            
            for doc_folder in chunks_dir.iterdir():
                if doc_folder.is_dir():
                    doc_name = doc_folder.name
                    logger.info(f"ğŸ“„ å¤„ç†æ–‡æ¡£: {doc_name}")
                    
                    # ç¡®å®šç›®æ ‡é›†åˆ
                    collection_id = self._determine_collection_id(doc_name)
                    logger.info(f"  ğŸ¯ ç›®æ ‡é›†åˆ: {collection_id}")
                    
                    # è¯»å–æ‰€æœ‰åˆ†å—æ–‡ä»¶
                    chunks = self._read_chunks_from_folder(doc_folder, doc_name)
                    logger.info(f"  ğŸ“Š è¯»å–åˆ° {len(chunks)} ä¸ªåˆ†å—")
                    
                    if chunks:
                        # åˆ†æ‰¹æ·»åŠ åˆ°é›†åˆ
                        self._add_chunks_to_collection(chunks, collection_id, doc_name)
            
            # 3. éªŒè¯ç»“æœ
            self._verify_build()
            
            logger.info("âœ… å¤šé›†åˆæ•°æ®åº“æ„å»ºå®Œæˆï¼")
            return True

        except Exception as e:
            logger.error(f"âŒ é‡å»ºå¤±è´¥: {e}")
            return False

    def build_auto(self):
        """è‡ªåŠ¨æ„å»ºæ•°æ®åº“ï¼ˆæ— äº¤äº’ï¼‰"""
        logger.info("ğŸš€ å¼€å§‹è‡ªåŠ¨æ„å»ºå¤šé›†åˆæ•°æ®åº“")

        try:
            # 1. åˆå§‹åŒ–æ£€ç´¢å™¨
            self._init_retriever()

            # 2. å¤„ç†æ¯ä¸ªæ–‡æ¡£ç±»å‹
            chunks_dir = Path("data/processed_docs/chunks")

            for doc_folder in chunks_dir.iterdir():
                if doc_folder.is_dir():
                    doc_name = doc_folder.name
                    logger.info(f"ğŸ“„ å¤„ç†æ–‡æ¡£: {doc_name}")

                    # ç¡®å®šç›®æ ‡é›†åˆ
                    collection_id = self._determine_collection_id(doc_name)
                    logger.info(f"  ğŸ¯ ç›®æ ‡é›†åˆ: {collection_id}")

                    # è¯»å–æ‰€æœ‰åˆ†å—æ–‡ä»¶
                    chunks = self._read_chunks_from_folder(doc_folder, doc_name)
                    logger.info(f"  ğŸ“Š è¯»å–åˆ° {len(chunks)} ä¸ªåˆ†å—")

                    if chunks:
                        # åˆ†æ‰¹æ·»åŠ åˆ°é›†åˆ
                        self._add_chunks_to_collection(chunks, collection_id, doc_name)

            # 3. éªŒè¯ç»“æœ
            self._verify_build()

            logger.info("âœ… è‡ªåŠ¨æ„å»ºå®Œæˆï¼")
            return True

        except Exception as e:
            logger.error(f"âŒ è‡ªåŠ¨æ„å»ºå¤±è´¥: {e}")
            return False
    
    def _init_retriever(self):
        """åˆå§‹åŒ–æ£€ç´¢å™¨"""
        logger.info("ğŸ”§ åˆå§‹åŒ–å¤šé›†åˆæ£€ç´¢å™¨...")
        
        # æ„å»ºæ£€ç´¢å™¨é…ç½®ï¼ˆä½¿ç”¨æ­£ç¡®çš„åµŒå¥—ç»“æ„ï¼‰
        retriever_config = {
            'chromadb': {
                'db_path': self.config_manager.get('retrieval.chromadb.db_path', './data/chroma_db'),
                'default_collection_name': self.config_manager.get('retrieval.chromadb.default_collection_name', 'knowledge_base')
            },
            'embedding': {
                'model_path': self.config_manager.get('retrieval.embedding.model_path'),
                'normalize_embeddings': self.config_manager.get('retrieval.embedding.normalize_embeddings', True)
            },
            'collections': self.config_manager.get('embedding.collections', [])
        }
        
        self.retriever = ChromaDBRetriever(retriever_config)
        logger.info("âœ… å¤šé›†åˆæ£€ç´¢å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _read_chunks_from_folder(self, folder_path: Path, doc_name: str) -> List[Dict[str, Any]]:
        """ä»æ–‡ä»¶å¤¹è¯»å–æ‰€æœ‰åˆ†å—"""
        chunks = []

        for chunk_file in folder_path.glob("*.md"):
            try:
                with open(chunk_file, 'r', encoding='utf-8') as f:
                    content = f.read()

                # è§£æYAMLå‰ç½®å…ƒæ•°æ®å’Œå†…å®¹
                yaml_metadata = {}
                actual_content = content

                if content.startswith('---'):
                    parts = content.split('---', 2)
                    if len(parts) >= 3:
                        yaml_content = parts[1].strip()
                        actual_content = parts[2].strip()

                        # è§£æYAMLå…ƒæ•°æ®
                        for line in yaml_content.split('\n'):
                            line = line.strip()
                            if ':' in line:
                                key, value = line.split(':', 1)
                                yaml_metadata[key.strip()] = value.strip()

                # æå–æºæ–‡æ¡£ä¿¡æ¯
                source_document = (
                    yaml_metadata.get('æºæ–‡æ¡£') or
                    yaml_metadata.get('æºæ–‡ä»¶') or
                    doc_name
                )

                # åˆ›å»ºåˆ†å—æ•°æ®
                chunk_data = {
                    'content': actual_content,
                    'metadata': {
                        'document': doc_name,
                        'source_document': source_document,
                        'file_path': str(chunk_file),
                        'chunk_id': chunk_file.stem,
                        **yaml_metadata  # åŒ…å«æ‰€æœ‰YAMLå…ƒæ•°æ®
                    },
                    'id': f"{doc_name}_{chunk_file.stem}"
                }

                chunks.append(chunk_data)

            except Exception as e:
                logger.warning(f"âš ï¸ è¯»å–åˆ†å—æ–‡ä»¶å¤±è´¥ {chunk_file}: {e}")

        return chunks
    
    def _add_chunks_to_collection(self, chunks: List[Dict[str, Any]], collection_id: str, doc_name: str):
        """åˆ†æ‰¹æ·»åŠ åˆ†å—åˆ°é›†åˆ"""
        batch_size = 30  # å‡å°æ‰¹å¤„ç†å¤§å°
        total_chunks = len(chunks)
        total_batches = (total_chunks + batch_size - 1) // batch_size
        
        for i in range(0, total_chunks, batch_size):
            end_idx = min(i + batch_size, total_chunks)
            batch_chunks = chunks[i:end_idx]
            
            current_batch = (i // batch_size) + 1
            logger.info(f"  ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {current_batch}/{total_batches} ({len(batch_chunks)} ä¸ªåˆ†å—)")
            
            # å‡†å¤‡æ‰¹æ¬¡æ•°æ®
            documents = [chunk['content'] for chunk in batch_chunks]
            metadatas = [chunk['metadata'] for chunk in batch_chunks]
            ids = [chunk['id'] for chunk in batch_chunks]
            
            try:
                # æ·»åŠ åˆ°é›†åˆ
                self.retriever.add_documents(
                    documents=documents,
                    metadatas=metadatas,
                    ids=ids,
                    collection_id=collection_id
                )
                
                logger.info(f"    âœ… æ‰¹æ¬¡ {current_batch} æ·»åŠ æˆåŠŸ")
                
            except Exception as e:
                logger.error(f"    âŒ æ‰¹æ¬¡ {current_batch} æ·»åŠ å¤±è´¥: {e}")
                # ç»§ç»­å¤„ç†ä¸‹ä¸€æ‰¹æ¬¡
        
        logger.info(f"âœ… {doc_name} â†’ {collection_id} é›†åˆå®Œæˆ ({total_chunks} ä¸ªåˆ†å—)")
    
    def _determine_collection_id(self, doc_name: str) -> str:
        """ç¡®å®šæ–‡æ¡£åº”è¯¥å­˜å‚¨åˆ°å“ªä¸ªé›†åˆ"""
        
        # ç²¾ç¡®åŒ¹é…
        if doc_name in self.doc_to_collection_mapping:
            return self.doc_to_collection_mapping[doc_name]
        
        # æ¨¡ç³ŠåŒ¹é…
        doc_lower = doc_name.lower()
        
        if 'äººæ°‘é“¶è¡Œ' in doc_name or 'é‡‘èç»Ÿè®¡' in doc_name:
            return 'pboc_statistics'
        elif '1104' in doc_name:
            if '2024' in doc_name:
                return 'report_1104_2024'
            elif '2022' in doc_name:
                return 'report_1104_2022'
            else:
                return 'report_1104_2024'  # é»˜è®¤æœ€æ–°ç‰ˆ
        elif 'east' in doc_lower:
            if 'å…ƒæ•°æ®' in doc_name or 'metadata' in doc_lower:
                return 'east_metadata'
            else:
                return 'east_data_structure'
        elif 'ä¸€è¡¨é€š' in doc_name:
            if 'æ˜ å°„' in doc_name or 'mapping' in doc_lower:
                return 'ybt_product_mapping'
            else:
                return 'ybt_data_structure'
        else:
            logger.warning(f"âš ï¸ æœªçŸ¥æ–‡æ¡£ç±»å‹ï¼Œå­˜å‚¨åˆ°é»˜è®¤é›†åˆ: {doc_name}")
            return 'default'
    
    def _verify_build(self):
        """éªŒè¯æ„å»ºç»“æœ"""
        logger.info("ğŸ” éªŒè¯æ„å»ºç»“æœ...")
        
        try:
            stats = self.retriever.get_stats()
            
            logger.info("ğŸ“Š æ„å»ºåç»Ÿè®¡:")
            logger.info(f"  æ€»æ–‡æ¡£æ•°: {stats['total_documents']}")
            
            for collection_id, collection_stats in stats['collections'].items():
                count = collection_stats['document_count']
                status = collection_stats['status']
                logger.info(f"  é›†åˆ {collection_id}: {count} ä¸ªæ–‡æ¡£ ({status})")
            
            if stats['total_documents'] > 0:
                logger.info("âœ… æ•°æ®åº“æ„å»ºéªŒè¯é€šè¿‡")
            else:
                logger.warning("âš ï¸ æ•°æ®åº“ä¸ºç©ºï¼Œè¯·æ£€æŸ¥åˆ†å—æ–‡ä»¶")
                
        except Exception as e:
            logger.error(f"âŒ éªŒè¯å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ CategoryRAGå¤šé›†åˆæ•°æ®åº“æ„å»ºå·¥å…·")
    print("=" * 50)
    
    # æ£€æŸ¥åˆ†å—æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    chunks_dir = Path("data/processed_docs/chunks")
    if not chunks_dir.exists():
        print(f"\nâŒ æœªæ‰¾åˆ°åˆ†å—ç›®å½•: {chunks_dir}")
        return
    
    print(f"ğŸ“‚ æ‰¾åˆ°åˆ†å—ç›®å½•: {chunks_dir}")
    
    # åˆ—å‡ºå¯ç”¨æ–‡æ¡£
    doc_folders = [f for f in chunks_dir.iterdir() if f.is_dir()]
    print(f"ğŸ“š å‘ç° {len(doc_folders)} ä¸ªæ–‡æ¡£ç±»å‹:")
    for folder in doc_folders:
        chunk_count = len(list(folder.glob("*.md")))
        print(f"  - {folder.name}: {chunk_count} ä¸ªåˆ†å—")
    
    # ç¡®è®¤é‡å»º
    response = input(f"\nâš ï¸ è¿™å°†é‡æ–°æ„å»ºæ•°æ®åº“ã€‚æ˜¯å¦ç»§ç»­ï¼Ÿ(y/N): ")
    if response.lower() != 'y':
        print("âŒ é‡å»ºå·²å–æ¶ˆ")
        return
    
    try:
        builder = CollectionDatabaseBuilder()
        success = builder.build()
        
        if success:
            print("\nğŸ‰ å¤šé›†åˆæ•°æ®åº“é‡å»ºå®Œæˆï¼")
            print("ğŸ’¡ ç°åœ¨å¯ä»¥ä½¿ç”¨æ–°çš„å¤šåº“æ™ºèƒ½æ£€ç´¢åŠŸèƒ½äº†")
        else:
            print("\nâŒ é‡å»ºå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
        
    except Exception as e:
        print(f"\nâŒ é‡å»ºå¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
