#!/usr/bin/env python3
"""
RAGç³»ç»Ÿç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–æ„å»ºè„šæœ¬
è§£å†³æ•°æ®æµæ°´çº¿ä¸­çš„æ‰‹åŠ¨å¹²é¢„æ–­ç‚¹ï¼Œå®ç°ä¸€é”®å¼æ„å»º
"""

import os
import sys
import logging
import argparse
import time
from pathlib import Path
from typing import Dict, Any, Optional

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(__file__))

# å¯¼å…¥ç°æœ‰æ¨¡å—
from Chunk import DocumentProcessingWorkflow
from rebuild_multi_collection_db import MultiCollectionBuilder

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('rag_system_build.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class RAGSystemBuilder:
    """RAGç³»ç»Ÿç«¯åˆ°ç«¯æ„å»ºå™¨"""
    
    def __init__(self, 
                 input_dir: str = "data/KnowledgeBase",
                 output_dir: str = "data/processed_docs",
                 chunk_size: int = 5000,
                 overlap_size: int = 1000,
                 auto_test: bool = True,
                 backup_existing: bool = True):
        """
        åˆå§‹åŒ–æ„å»ºå™¨
        
        Args:
            input_dir: è¾“å…¥æ–‡æ¡£ç›®å½•
            output_dir: è¾“å‡ºç›®å½•
            chunk_size: åˆ†å—å¤§å°
            overlap_size: é‡å å¤§å°
            auto_test: æ˜¯å¦è‡ªåŠ¨æµ‹è¯•
            backup_existing: æ˜¯å¦å¤‡ä»½ç°æœ‰æ•°æ®
        """
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.chunk_size = chunk_size
        self.overlap_size = overlap_size
        self.auto_test = auto_test
        self.backup_existing = backup_existing
        
        # æ„å»ºç»Ÿè®¡
        self.build_stats = {
            'start_time': None,
            'end_time': None,
            'total_documents': 0,
            'total_chunks': 0,
            'total_collections': 0,
            'errors': [],
            'stages_completed': []
        }
        
        logger.info("ğŸ¯ RAGç³»ç»Ÿæ„å»ºå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def build(self) -> bool:
        """æ‰§è¡Œå®Œæ•´çš„ç«¯åˆ°ç«¯æ„å»º"""
        logger.info("ğŸš€ å¼€å§‹RAGç³»ç»Ÿç«¯åˆ°ç«¯æ„å»º")
        self.build_stats['start_time'] = time.time()
        
        try:
            # é˜¶æ®µ0ï¼šç¯å¢ƒæ£€æŸ¥
            if not self._check_environment():
                return False
            self.build_stats['stages_completed'].append('environment_check')
            
            # é˜¶æ®µ1ï¼šæ–‡æ¡£å¤„ç†ï¼ˆè½¬æ¢+åˆ†å—ï¼‰
            if not self._process_documents():
                return False
            self.build_stats['stages_completed'].append('document_processing')
            
            # é˜¶æ®µ2ï¼šå¤šé›†åˆæ•°æ®åº“æ„å»º
            if not self._build_vector_database():
                return False
            self.build_stats['stages_completed'].append('vector_database')
            
            # é˜¶æ®µ3ï¼šç³»ç»ŸéªŒè¯ï¼ˆå¯é€‰ï¼‰
            if self.auto_test:
                if not self._verify_system():
                    logger.warning("âš ï¸ ç³»ç»ŸéªŒè¯å¤±è´¥ï¼Œä½†æ„å»ºå·²å®Œæˆ")
                else:
                    self.build_stats['stages_completed'].append('system_verification')
            
            # ç”Ÿæˆæ„å»ºæŠ¥å‘Š
            self._generate_build_report()
            
            self.build_stats['end_time'] = time.time()
            logger.info("âœ… RAGç³»ç»Ÿæ„å»ºå®Œæˆï¼")
            return True
            
        except Exception as e:
            self.build_stats['errors'].append(str(e))
            logger.error(f"âŒ RAGç³»ç»Ÿæ„å»ºå¤±è´¥: {e}")
            return False
    
    def _check_environment(self) -> bool:
        """æ£€æŸ¥æ„å»ºç¯å¢ƒ"""
        logger.info("ğŸ” æ£€æŸ¥æ„å»ºç¯å¢ƒ...")
        
        # æ£€æŸ¥è¾“å…¥ç›®å½•
        if not self.input_dir.exists():
            logger.error(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {self.input_dir}")
            return False
        
        # æ£€æŸ¥è¾“å…¥æ–‡æ¡£
        supported_formats = ['.pdf', '.docx', '.xlsx', '.txt', '.md']
        input_files = []
        for fmt in supported_formats:
            input_files.extend(list(self.input_dir.glob(f"*{fmt}")))
        
        if not input_files:
            logger.error(f"âŒ è¾“å…¥ç›®å½•ä¸­æœªæ‰¾åˆ°æ”¯æŒçš„æ–‡æ¡£æ ¼å¼: {supported_formats}")
            return False
        
        logger.info(f"ğŸ“„ å‘ç° {len(input_files)} ä¸ªå¾…å¤„ç†æ–‡æ¡£")
        self.build_stats['total_documents'] = len(input_files)
        
        # æ£€æŸ¥ä¾èµ–æ¨¡å—
        try:
            import markitdown
            logger.info("âœ… markitdown æ¨¡å—å¯ç”¨")
        except ImportError:
            logger.error("âŒ markitdown æ¨¡å—æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install markitdown")
            return False
        
        try:
            import chromadb
            logger.info("âœ… chromadb æ¨¡å—å¯ç”¨")
        except ImportError:
            logger.error("âŒ chromadb æ¨¡å—æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install chromadb")
            return False
        
        # æ£€æŸ¥BGEæ¨¡å‹
        try:
            sys.path.insert(0, 'src')
            from src.config import ConfigManager
            config = ConfigManager()
            model_path = config.get('retrieval.embedding.model_path')
            
            if model_path and Path(model_path).exists():
                logger.info(f"âœ… BGEæ¨¡å‹è·¯å¾„æœ‰æ•ˆ: {model_path}")
            else:
                logger.warning(f"âš ï¸ BGEæ¨¡å‹è·¯å¾„æ— æ•ˆï¼Œå°†ä½¿ç”¨é»˜è®¤embedding: {model_path}")
        except Exception as e:
            logger.warning(f"âš ï¸ é…ç½®æ£€æŸ¥å¤±è´¥ï¼Œå°†ä½¿ç”¨é»˜è®¤è®¾ç½®: {e}")
        
        logger.info("âœ… ç¯å¢ƒæ£€æŸ¥é€šè¿‡")
        return True
    
    def _process_documents(self) -> bool:
        """å¤„ç†æ–‡æ¡£ï¼ˆè½¬æ¢+åˆ†å—ï¼‰"""
        logger.info("ğŸ“ å¼€å§‹æ–‡æ¡£å¤„ç†é˜¶æ®µ...")
        
        try:
            # åˆ›å»ºæ–‡æ¡£å¤„ç†å·¥ä½œæµ
            workflow = DocumentProcessingWorkflow(
                input_dir=str(self.input_dir),
                output_base_dir=str(self.output_dir),
                chunk_size=self.chunk_size,
                overlap_size=self.overlap_size
            )
            
            # æ‰§è¡Œå®Œæ•´å·¥ä½œæµ
            results = workflow.run_full_workflow()
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.build_stats['total_chunks'] = results.get('text_chunks', 0) + results.get('excel_chunks', 0)
            
            if results.get('errors'):
                logger.warning(f"âš ï¸ æ–‡æ¡£å¤„ç†è¿‡ç¨‹ä¸­å‡ºç° {len(results['errors'])} ä¸ªé”™è¯¯")
                self.build_stats['errors'].extend(results['errors'])
            
            logger.info(f"âœ… æ–‡æ¡£å¤„ç†å®Œæˆ: {results['converted_files']} ä¸ªæ–‡æ¡£, {self.build_stats['total_chunks']} ä¸ªåˆ†å—")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {e}")
            self.build_stats['errors'].append(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {e}")
            return False
    
    def _build_vector_database(self) -> bool:
        """æ„å»ºå‘é‡æ•°æ®åº“"""
        logger.info("ğŸ—„ï¸ å¼€å§‹å‘é‡æ•°æ®åº“æ„å»º...")
        
        try:
            # æ£€æŸ¥åˆ†å—æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            chunks_dir = self.output_dir / "chunks"
            if not chunks_dir.exists():
                logger.error(f"âŒ åˆ†å—ç›®å½•ä¸å­˜åœ¨: {chunks_dir}")
                return False
            
            # åˆ›å»ºå¤šé›†åˆæ„å»ºå™¨
            builder = MultiCollectionBuilder()
            
            # æ‰§è¡Œæ„å»º
            success = builder.build(str(chunks_dir))
            
            if success:
                # è·å–æ„å»ºç»Ÿè®¡
                try:
                    sys.path.insert(0, 'src')
                    from src.config import ConfigManager
                    from src.retrievers import ChromaDBRetriever
                    
                    config_manager = ConfigManager()
                    chromadb_config = config_manager.get_section('retrieval.chromadb')
                    embedding_config = config_manager.get_section('retrieval.embedding')
                    collections_config = config_manager.get('retrieval.collections', [])
                    
                    retriever_config = {
                        'db_path': chromadb_config.get('db_path', './data/chroma_db'),
                        'default_collection_name': chromadb_config.get('default_collection_name', 'knowledge_base'),
                        'model_path': embedding_config.get('model_path'),
                        'normalize_embeddings': embedding_config.get('normalize_embeddings', True),
                        'collections': collections_config
                    }
                    
                    retriever = ChromaDBRetriever(retriever_config)
                    stats = retriever.get_stats()
                    
                    self.build_stats['total_collections'] = len(stats.get('collections', {}))
                    
                    logger.info(f"âœ… å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆ: {self.build_stats['total_collections']} ä¸ªé›†åˆ")
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ æ— æ³•è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯: {e}")
                
                return True
            else:
                logger.error("âŒ å‘é‡æ•°æ®åº“æ„å»ºå¤±è´¥")
                return False
                
        except Exception as e:
            logger.error(f"âŒ å‘é‡æ•°æ®åº“æ„å»ºå¼‚å¸¸: {e}")
            self.build_stats['errors'].append(f"å‘é‡æ•°æ®åº“æ„å»ºå¤±è´¥: {e}")
            return False
    
    def _verify_system(self) -> bool:
        """éªŒè¯ç³»ç»ŸåŠŸèƒ½"""
        logger.info("ğŸ§ª å¼€å§‹ç³»ç»ŸéªŒè¯...")
        
        try:
            # ç®€å•çš„ç³»ç»ŸéªŒè¯
            sys.path.insert(0, 'src')
            from src import RAGSystem, ConfigManager
            
            # åˆå§‹åŒ–RAGç³»ç»Ÿ
            config_manager = ConfigManager()
            rag_system = RAGSystem(config_manager)
            
            # æ‰§è¡Œå¥åº·æ£€æŸ¥
            health_status = rag_system.health_check()
            
            if health_status.get('system') == 'healthy':
                logger.info("âœ… ç³»ç»ŸéªŒè¯é€šè¿‡")
                return True
            else:
                logger.warning(f"âš ï¸ ç³»ç»Ÿå¥åº·æ£€æŸ¥å¼‚å¸¸: {health_status}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ ç³»ç»ŸéªŒè¯å¤±è´¥: {e}")
            self.build_stats['errors'].append(f"ç³»ç»ŸéªŒè¯å¤±è´¥: {e}")
            return False
    
    def _generate_build_report(self):
        """ç”Ÿæˆæ„å»ºæŠ¥å‘Š"""
        logger.info("ğŸ“Š ç”Ÿæˆæ„å»ºæŠ¥å‘Š...")
        
        duration = self.build_stats['end_time'] - self.build_stats['start_time']
        
        report = f"""
# RAGç³»ç»Ÿæ„å»ºæŠ¥å‘Š

## æ„å»ºæ¦‚è¦
- å¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(self.build_stats['start_time']))}
- ç»“æŸæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(self.build_stats['end_time']))}
- æ€»è€—æ—¶: {duration:.2f} ç§’

## å¤„ç†ç»Ÿè®¡
- å¤„ç†æ–‡æ¡£: {self.build_stats['total_documents']} ä¸ª
- ç”Ÿæˆåˆ†å—: {self.build_stats['total_chunks']} ä¸ª
- åˆ›å»ºé›†åˆ: {self.build_stats['total_collections']} ä¸ª

## å®Œæˆé˜¶æ®µ
{chr(10).join(f"- {stage}" for stage in self.build_stats['stages_completed'])}

## é”™è¯¯ä¿¡æ¯
{chr(10).join(f"- {error}" for error in self.build_stats['errors']) if self.build_stats['errors'] else "æ— é”™è¯¯"}

## æ„å»ºçŠ¶æ€
{'âœ… æ„å»ºæˆåŠŸ' if len(self.build_stats['stages_completed']) >= 3 else 'âŒ æ„å»ºä¸å®Œæ•´'}
"""
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.output_dir / "build_report.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"ğŸ“‹ æ„å»ºæŠ¥å‘Šå·²ä¿å­˜: {report_file}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="RAGç³»ç»Ÿç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–æ„å»ºå·¥å…·")
    parser.add_argument("--input", "-i", default="data/KnowledgeBase", help="è¾“å…¥æ–‡æ¡£ç›®å½•")
    parser.add_argument("--output", "-o", default="data/processed_docs", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--chunk-size", "-c", type=int, default=5000, help="åˆ†å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰")
    parser.add_argument("--overlap-size", "-ol", type=int, default=1000, help="é‡å å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰")
    parser.add_argument("--no-test", action="store_true", help="è·³è¿‡ç³»ç»ŸéªŒè¯")
    parser.add_argument("--no-backup", action="store_true", help="ä¸å¤‡ä»½ç°æœ‰æ•°æ®")
    
    args = parser.parse_args()
    
    print("ğŸ¯ RAGç³»ç»Ÿç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–æ„å»ºå·¥å…·")
    print("=" * 60)
    print(f"è¾“å…¥ç›®å½•: {args.input}")
    print(f"è¾“å‡ºç›®å½•: {args.output}")
    print(f"åˆ†å—å‚æ•°: {args.chunk_size}å­—ç¬¦/å—, {args.overlap_size}å­—ç¬¦é‡å ")
    print(f"è‡ªåŠ¨æµ‹è¯•: {'å¦' if args.no_test else 'æ˜¯'}")
    print(f"å¤‡ä»½æ•°æ®: {'å¦' if args.no_backup else 'æ˜¯'}")
    
    # ç¡®è®¤æ„å»º
    response = input(f"\nâš ï¸ è¿™å°†æ‰§è¡Œå®Œæ•´çš„RAGç³»ç»Ÿæ„å»ºæµç¨‹ã€‚æ˜¯å¦ç»§ç»­ï¼Ÿ(y/N): ")
    if response.lower() != 'y':
        print("âŒ æ„å»ºå·²å–æ¶ˆ")
        return
    
    try:
        # åˆ›å»ºæ„å»ºå™¨
        builder = RAGSystemBuilder(
            input_dir=args.input,
            output_dir=args.output,
            chunk_size=args.chunk_size,
            overlap_size=args.overlap_size,
            auto_test=not args.no_test,
            backup_existing=not args.no_backup
        )
        
        # æ‰§è¡Œæ„å»º
        success = builder.build()
        
        if success:
            print("\nğŸ‰ RAGç³»ç»Ÿæ„å»ºå®Œæˆï¼")
            print("ğŸš€ ç°åœ¨å¯ä»¥å¯åŠ¨ç³»ç»Ÿ: python3 rag_app.py")
        else:
            print("\nâŒ RAGç³»ç»Ÿæ„å»ºå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
            sys.exit(1)
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­æ„å»º")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ æ„å»ºå¼‚å¸¸: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
