"""
æ–‡æœ¬åˆ†å—å™¨ - å°†Markdownæ–‡æ¡£æ™ºèƒ½åˆ†å—
"""

import re
import logging
from pathlib import Path
from typing import List, Dict, Any

logger = logging.getLogger(__name__)

class TextChunker:
    """æ–‡æœ¬åˆ†å—å™¨ - æŒ‰è¯­ä¹‰å’Œç»“æ„åˆ†å—"""
    
    def __init__(self, chunk_size: int = 1000, overlap_size: int = 200):
        """
        åˆå§‹åŒ–æ–‡æœ¬åˆ†å—å™¨
        
        Args:
            chunk_size: åˆ†å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            overlap_size: é‡å å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
        """
        self.chunk_size = chunk_size
        self.overlap_size = overlap_size
        logger.info(f"âœ… æ–‡æœ¬åˆ†å—å™¨å·²åˆå§‹åŒ– (å—å¤§å°: {chunk_size}, é‡å : {overlap_size})")
    
    def chunk_document(self, markdown_file: str, output_dir: str) -> List[str]:
        """
        åˆ†å—Markdownæ–‡æ¡£
        
        Args:
            markdown_file: Markdownæ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
            
        Returns:
            ç”Ÿæˆçš„åˆ†å—æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        input_path = Path(markdown_file)
        output_path = Path(output_dir)
        
        if not input_path.exists():
            logger.error(f"âŒ Markdownæ–‡ä»¶ä¸å­˜åœ¨: {markdown_file}")
            return []
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_path.mkdir(parents=True, exist_ok=True)
        
        try:
            # è¯»å–Markdownå†…å®¹
            with open(input_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # åˆ†å—å¤„ç†
            chunks = self._split_content(content)
            
            # ç”Ÿæˆåˆ†å—æ–‡ä»¶
            chunk_files = []
            for i, chunk in enumerate(chunks):
                chunk_file = output_path / f"{input_path.stem}_chunk_{i+1}.md"
                
                # åˆ›å»ºåˆ†å—å†…å®¹
                chunk_content = self._create_chunk_content(
                    input_path.name, chunk, i+1, len(chunks)
                )
                
                # ä¿å­˜åˆ†å—æ–‡ä»¶
                with open(chunk_file, 'w', encoding='utf-8') as f:
                    f.write(chunk_content)
                
                chunk_files.append(str(chunk_file))
                logger.debug(f"ğŸ“ åˆ›å»ºåˆ†å—: {chunk_file.name}")
            
            logger.info(f"âœ… æ–‡æ¡£åˆ†å—å®Œæˆ: {len(chunk_files)}ä¸ªåˆ†å—")
            return chunk_files
            
        except Exception as e:
            logger.error(f"âŒ æ–‡æ¡£åˆ†å—å¤±è´¥ {markdown_file}: {e}")
            return []
    
    def _split_content(self, content: str) -> List[str]:
        """åˆ†å‰²å†…å®¹ä¸ºå¤šä¸ªå—"""
        chunks = []
        
        # é¦–å…ˆå°è¯•æŒ‰æ ‡é¢˜åˆ†å‰²
        header_chunks = self._split_by_headers(content)
        
        # å¦‚æœæŒ‰æ ‡é¢˜åˆ†å‰²çš„å—å¤ªå¤§ï¼Œå†æŒ‰æ®µè½åˆ†å‰²
        for header_chunk in header_chunks:
            if len(header_chunk) <= self.chunk_size:
                chunks.append(header_chunk)
            else:
                # å—å¤ªå¤§ï¼ŒæŒ‰æ®µè½è¿›ä¸€æ­¥åˆ†å‰²
                paragraph_chunks = self._split_by_paragraphs(header_chunk)
                chunks.extend(paragraph_chunks)
        
        return chunks
    
    def _split_by_headers(self, content: str) -> List[str]:
        """æŒ‰æ ‡é¢˜åˆ†å‰²å†…å®¹"""
        # åŒ¹é…Markdownæ ‡é¢˜
        header_pattern = r'^(#{1,6}\s+.+)$'
        lines = content.split('\n')
        
        chunks = []
        current_chunk = []
        
        for line in lines:
            if re.match(header_pattern, line, re.MULTILINE) and current_chunk:
                # é‡åˆ°æ–°æ ‡é¢˜ï¼Œä¿å­˜å½“å‰å—
                chunks.append('\n'.join(current_chunk))
                current_chunk = [line]
            else:
                current_chunk.append(line)
        
        # ä¿å­˜æœ€åä¸€ä¸ªå—
        if current_chunk:
            chunks.append('\n'.join(current_chunk))
        
        return chunks
    
    def _split_by_paragraphs(self, content: str) -> List[str]:
        """æŒ‰æ®µè½åˆ†å‰²å†…å®¹"""
        # æŒ‰åŒæ¢è¡Œç¬¦åˆ†å‰²æ®µè½
        paragraphs = content.split('\n\n')
        
        chunks = []
        current_chunk = []
        current_size = 0
        
        for paragraph in paragraphs:
            paragraph_size = len(paragraph)
            
            if current_size + paragraph_size > self.chunk_size and current_chunk:
                # å½“å‰å—å·²æ»¡ï¼Œä¿å­˜å¹¶å¼€å§‹æ–°å—
                chunks.append('\n\n'.join(current_chunk))
                
                # æ–°å—ä»é‡å éƒ¨åˆ†å¼€å§‹
                if self.overlap_size > 0 and current_chunk:
                    overlap_text = '\n\n'.join(current_chunk)[-self.overlap_size:]
                    current_chunk = [overlap_text, paragraph]
                    current_size = len(overlap_text) + paragraph_size
                else:
                    current_chunk = [paragraph]
                    current_size = paragraph_size
            else:
                current_chunk.append(paragraph)
                current_size += paragraph_size
        
        # ä¿å­˜æœ€åä¸€ä¸ªå—
        if current_chunk:
            chunks.append('\n\n'.join(current_chunk))
        
        return chunks
    
    def _create_chunk_content(self, source_file: str, chunk_text: str, 
                             chunk_num: int, total_chunks: int) -> str:
        """åˆ›å»ºåˆ†å—å†…å®¹"""
        content = f"""---
æºæ–‡æ¡£: {source_file}
åˆ†å—ç¼–å·: {chunk_num}/{total_chunks}
åˆ†å—ID: {Path(source_file).stem}_chunk_{chunk_num}
---

{chunk_text}
"""
        return content
