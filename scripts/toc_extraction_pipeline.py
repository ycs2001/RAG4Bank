#!/usr/bin/env python3
"""
CategoryRAGæ–‡æ¡£ç›®å½•æå–è‡ªåŠ¨åŒ–Pipeline
ä¸€é”®å®Œæˆæ‰€æœ‰æ–‡æ¡£çš„ç›®å½•æå–ã€éªŒè¯å’ŒæŠ¥å‘Šç”Ÿæˆ
"""

import os
import sys
import time
import json
import yaml
import logging
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / 'src'))

from src.config import EnhancedConfigManager
from src.core.document_preprocessor import DocumentPreprocessor
from src.llm.deepseek_llm import DeepSeekLLM
from src.core.semantic_enhancer import SemanticEnhancer

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/toc_pipeline.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class DocumentResult:
    """æ–‡æ¡£å¤„ç†ç»“æœ"""
    doc_id: str
    title: str
    file_path: str
    status: str
    chapters_count: int
    subsections_count: int
    confidence: float
    processing_time: float
    error_message: Optional[str] = None
    table_numbers: List[str] = None

class TOCExtractionPipeline:
    """æ–‡æ¡£ç›®å½•æå–Pipeline"""
    
    def __init__(self):
        """åˆå§‹åŒ–Pipeline"""
        self.config_manager = ConfigManager()
        self.results: List[DocumentResult] = []
        self.start_time = datetime.now()
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        Path('logs').mkdir(exist_ok=True)
        Path('data/toc').mkdir(parents=True, exist_ok=True)
        
        logger.info("ğŸš€ CategoryRAGæ–‡æ¡£ç›®å½•æå–Pipelineå¯åŠ¨")
        logger.info("=" * 80)
    
    def initialize_components(self) -> bool:
        """åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶"""
        try:
            logger.info("ğŸ”§ åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶...")
            
            # åˆå§‹åŒ–DeepSeek LLM
            deepseek_config = self.config_manager.get('llm.deepseek', {})
            if not deepseek_config.get('api_key'):
                logger.error("âŒ DeepSeek APIå¯†é’¥æœªé…ç½®")
                return False
            
            self.llm = DeepSeekLLM(deepseek_config)
            logger.info("âœ… DeepSeek LLMåˆå§‹åŒ–å®Œæˆ")
            
            # åˆå§‹åŒ–æ–‡æ¡£é¢„å¤„ç†å™¨
            self.preprocessor = DocumentPreprocessor(self.config_manager, self.llm)
            if not self.preprocessor.enabled:
                logger.error("âŒ æ–‡æ¡£é¢„å¤„ç†å™¨æœªå¯ç”¨")
                return False
            
            logger.info("âœ… æ–‡æ¡£é¢„å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
            
            # åˆå§‹åŒ–è¯­ä¹‰å¢å¼ºå™¨
            self.enhancer = SemanticEnhancer(self.config_manager, self.llm)
            logger.info("âœ… è¯­ä¹‰å¢å¼ºå™¨åˆå§‹åŒ–å®Œæˆ")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def cleanup_old_data(self):
        """æ¸…ç†æ—§çš„ç›®å½•æ•°æ®"""
        logger.info("ğŸ§¹ æ¸…ç†æ—§çš„ç›®å½•æ•°æ®...")
        
        toc_dir = Path('data/toc')
        if toc_dir.exists():
            yaml_files = list(toc_dir.glob('*.yaml'))
            json_files = list(toc_dir.glob('*.json'))
            
            for file in yaml_files + json_files:
                try:
                    file.unlink()
                    logger.debug(f"åˆ é™¤æ–‡ä»¶: {file}")
                except Exception as e:
                    logger.warning(f"åˆ é™¤æ–‡ä»¶å¤±è´¥ {file}: {e}")
            
            logger.info(f"âœ… æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {len(yaml_files + json_files)} ä¸ªæ—§æ–‡ä»¶")
        else:
            logger.info("ğŸ“ ç›®å½•æ•°æ®æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œè·³è¿‡æ¸…ç†")
    
    def get_configured_documents(self) -> Dict[str, Dict[str, Any]]:
        """è·å–é…ç½®çš„æ–‡æ¡£åˆ—è¡¨"""
        toc_config = self.config_manager.get('documents.toc', {})
        logger.info(f"ğŸ“‹ å‘ç° {len(toc_config)} ä¸ªé…ç½®çš„æ–‡æ¡£")
        
        for doc_id, doc_info in toc_config.items():
            title = doc_info.get('title', 'æœªçŸ¥æ ‡é¢˜')
            file_path = doc_info.get('file_path', 'æœªçŸ¥è·¯å¾„')
            exists = "âœ…" if Path(file_path).exists() else "âŒ"
            logger.info(f"  â€¢ {doc_id}: {title} {exists}")
        
        return toc_config
    
    def extract_single_document(self, doc_id: str, doc_info: Dict[str, Any]) -> DocumentResult:
        """æå–å•ä¸ªæ–‡æ¡£çš„ç›®å½•"""
        title = doc_info.get('title', 'æœªçŸ¥æ ‡é¢˜')
        file_path = doc_info.get('file_path', '')
        
        logger.info(f"ğŸ“„ å¼€å§‹å¤„ç†: {doc_id} - {title}")
        logger.info(f"ğŸ“ æ–‡ä»¶è·¯å¾„: {file_path}")
        
        start_time = time.time()
        
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not Path(file_path).exists():
                error_msg = f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
                logger.error(f"âŒ {error_msg}")
                return DocumentResult(
                    doc_id=doc_id,
                    title=title,
                    file_path=file_path,
                    status="file_not_found",
                    chapters_count=0,
                    subsections_count=0,
                    confidence=0.0,
                    processing_time=time.time() - start_time,
                    error_message=error_msg
                )
            
            # æ‰§è¡Œç›®å½•æå–
            result = self.preprocessor.extract_document_toc(doc_id, file_path)
            processing_time = time.time() - start_time
            
            # åˆ†æç»“æœ
            status = result.get('status', 'unknown')
            chapters = result.get('chapters', [])
            confidence = result.get('confidence', 0.0)
            
            # ç»Ÿè®¡å­ç« èŠ‚æ•°é‡å’Œè¡¨æ ¼ç¼–å·
            subsections_count = 0
            table_numbers = []
            
            for chapter in chapters:
                subsections = chapter.get('subsections', [])
                subsections_count += len(subsections)
                
                # æå–è¡¨æ ¼ç¼–å·
                chapter_num = chapter.get('chapter_num', '')
                if chapter_num and any(c.isdigit() or c in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ' for c in chapter_num):
                    table_numbers.append(chapter_num)
                
                for subsection in subsections:
                    sub_num = subsection.get('chapter_num', '')
                    if sub_num and any(c.isdigit() or c in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ' for c in sub_num):
                        table_numbers.append(sub_num)
            
            # åˆ›å»ºç»“æœå¯¹è±¡
            doc_result = DocumentResult(
                doc_id=doc_id,
                title=title,
                file_path=file_path,
                status=status,
                chapters_count=len(chapters),
                subsections_count=subsections_count,
                confidence=confidence,
                processing_time=processing_time,
                table_numbers=table_numbers
            )
            
            # è®°å½•ç»“æœ
            if status == 'completed':
                logger.info(f"âœ… æå–æˆåŠŸ: {len(chapters)}ä¸ªç« èŠ‚, {subsections_count}ä¸ªå­é¡¹, ç½®ä¿¡åº¦: {confidence:.2f}")
                if table_numbers:
                    logger.info(f"ğŸ“Š è¯†åˆ«çš„è¡¨æ ¼ç¼–å·: {table_numbers[:10]}{'...' if len(table_numbers) > 10 else ''}")
            else:
                logger.warning(f"âš ï¸ æå–çŠ¶æ€: {status}, ç½®ä¿¡åº¦: {confidence:.2f}")
            
            logger.info(f"â±ï¸ å¤„ç†è€—æ—¶: {processing_time:.1f}ç§’")
            
            return doc_result
            
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = f"å¤„ç†å¼‚å¸¸: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            
            return DocumentResult(
                doc_id=doc_id,
                title=title,
                file_path=file_path,
                status="error",
                chapters_count=0,
                subsections_count=0,
                confidence=0.0,
                processing_time=processing_time,
                error_message=error_msg
            )
    
    def run_extraction_pipeline(self, target_docs: Optional[List[str]] = None) -> bool:
        """è¿è¡Œæå–Pipeline"""
        logger.info("ğŸ”„ å¼€å§‹æ‰¹é‡æ–‡æ¡£ç›®å½•æå–...")
        
        # è·å–è¦å¤„ç†çš„æ–‡æ¡£
        all_docs = self.get_configured_documents()
        
        if target_docs:
            docs_to_process = {k: v for k, v in all_docs.items() if k in target_docs}
            logger.info(f"ğŸ¯ æŒ‡å®šå¤„ç† {len(docs_to_process)} ä¸ªæ–‡æ¡£")
        else:
            docs_to_process = all_docs
            logger.info(f"ğŸ“š å¤„ç†æ‰€æœ‰ {len(docs_to_process)} ä¸ªæ–‡æ¡£")
        
        # é€ä¸ªå¤„ç†æ–‡æ¡£
        for i, (doc_id, doc_info) in enumerate(docs_to_process.items(), 1):
            logger.info(f"\n{'='*60}")
            logger.info(f"ğŸ“– å¤„ç†è¿›åº¦: {i}/{len(docs_to_process)} - {doc_id}")
            logger.info(f"{'='*60}")
            
            result = self.extract_single_document(doc_id, doc_info)
            self.results.append(result)
            
            # çŸ­æš‚ä¼‘æ¯ï¼Œé¿å…APIé™åˆ¶
            if i < len(docs_to_process):
                logger.info("â³ ç­‰å¾…2ç§’...")
                time.sleep(2)
        
        return True

    def validate_extraction_results(self) -> Dict[str, Any]:
        """éªŒè¯æå–ç»“æœè´¨é‡"""
        logger.info("\nğŸ” éªŒè¯æå–ç»“æœè´¨é‡...")
        logger.info("=" * 60)

        validation_report = {
            'total_documents': len(self.results),
            'successful_extractions': 0,
            'failed_extractions': 0,
            'quality_issues': [],
            'statistics': {}
        }

        for result in self.results:
            if result.status == 'completed':
                validation_report['successful_extractions'] += 1

                # è´¨é‡æ£€æŸ¥
                quality_issues = []

                # æ£€æŸ¥ç« èŠ‚æ•°é‡
                if result.doc_id.startswith('report_1104') and result.chapters_count < 15:
                    quality_issues.append(f"1104æŠ¥è¡¨ç« èŠ‚æ•°é‡åå°‘: {result.chapters_count} < 15")

                if result.doc_id == 'pboc_statistics' and result.chapters_count < 20:
                    quality_issues.append(f"äººæ°‘é“¶è¡Œç»Ÿè®¡åˆ¶åº¦ç« èŠ‚æ•°é‡åå°‘: {result.chapters_count} < 20")

                # æ£€æŸ¥ç½®ä¿¡åº¦
                if result.confidence < 0.7:
                    quality_issues.append(f"ç½®ä¿¡åº¦åä½: {result.confidence:.2f} < 0.7")

                # æ£€æŸ¥è¡¨æ ¼ç¼–å·
                if result.table_numbers and len(result.table_numbers) < 5:
                    quality_issues.append(f"è¯†åˆ«çš„è¡¨æ ¼ç¼–å·åå°‘: {len(result.table_numbers)} < 5")

                if quality_issues:
                    validation_report['quality_issues'].append({
                        'doc_id': result.doc_id,
                        'issues': quality_issues
                    })

                # ç»Ÿè®¡ä¿¡æ¯
                validation_report['statistics'][result.doc_id] = {
                    'chapters': result.chapters_count,
                    'subsections': result.subsections_count,
                    'confidence': result.confidence,
                    'table_numbers_count': len(result.table_numbers) if result.table_numbers else 0,
                    'processing_time': result.processing_time
                }

            else:
                validation_report['failed_extractions'] += 1

        # è¾“å‡ºéªŒè¯ç»“æœ
        success_rate = validation_report['successful_extractions'] / validation_report['total_documents'] * 100
        logger.info(f"ğŸ“Š æå–æˆåŠŸç‡: {success_rate:.1f}% ({validation_report['successful_extractions']}/{validation_report['total_documents']})")

        if validation_report['quality_issues']:
            logger.warning(f"âš ï¸ å‘ç° {len(validation_report['quality_issues'])} ä¸ªè´¨é‡é—®é¢˜:")
            for issue in validation_report['quality_issues']:
                logger.warning(f"  â€¢ {issue['doc_id']}: {', '.join(issue['issues'])}")
        else:
            logger.info("âœ… æ‰€æœ‰æ–‡æ¡£è´¨é‡æ£€æŸ¥é€šè¿‡")

        return validation_report

    def generate_summary_report(self, validation_report: Dict[str, Any]):
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
        logger.info("\nğŸ“‹ ç”Ÿæˆå¤„ç†ç»“æœæ±‡æ€»æŠ¥å‘Š...")
        logger.info("=" * 80)

        total_time = (datetime.now() - self.start_time).total_seconds()

        # æ§åˆ¶å°æŠ¥å‘Š
        logger.info(f"ğŸ• Pipelineæ€»è€—æ—¶: {total_time:.1f}ç§’")
        logger.info(f"ğŸ“š å¤„ç†æ–‡æ¡£æ•°é‡: {len(self.results)}")
        logger.info(f"âœ… æˆåŠŸæå–: {validation_report['successful_extractions']}")
        logger.info(f"âŒ æå–å¤±è´¥: {validation_report['failed_extractions']}")

        logger.info("\nğŸ“Š è¯¦ç»†ç»Ÿè®¡:")
        logger.info("-" * 80)
        logger.info(f"{'æ–‡æ¡£ID':<20} {'ç« èŠ‚æ•°':<8} {'å­é¡¹æ•°':<8} {'ç½®ä¿¡åº¦':<8} {'è¡¨æ ¼æ•°':<8} {'è€—æ—¶(s)':<8} {'çŠ¶æ€'}")
        logger.info("-" * 80)

        for result in self.results:
            table_count = len(result.table_numbers) if result.table_numbers else 0
            status_icon = "âœ…" if result.status == 'completed' else "âŒ"

            logger.info(f"{result.doc_id:<20} {result.chapters_count:<8} {result.subsections_count:<8} "
                       f"{result.confidence:<8.2f} {table_count:<8} {result.processing_time:<8.1f} {status_icon}")

        # ä¿å­˜è¯¦ç»†æŠ¥å‘Šåˆ°æ–‡ä»¶
        report_data = {
            'pipeline_info': {
                'start_time': self.start_time.isoformat(),
                'end_time': datetime.now().isoformat(),
                'total_duration_seconds': total_time,
                'processed_documents': len(self.results)
            },
            'validation_report': validation_report,
            'detailed_results': []
        }

        for result in self.results:
            report_data['detailed_results'].append({
                'doc_id': result.doc_id,
                'title': result.title,
                'file_path': result.file_path,
                'status': result.status,
                'chapters_count': result.chapters_count,
                'subsections_count': result.subsections_count,
                'confidence': result.confidence,
                'processing_time': result.processing_time,
                'table_numbers': result.table_numbers,
                'error_message': result.error_message
            })

        # ä¿å­˜æŠ¥å‘Š
        report_file = f"logs/toc_pipeline_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)

        logger.info(f"\nğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

    def test_semantic_enhancement(self) -> bool:
        """æµ‹è¯•è¯­ä¹‰å¢å¼ºåŠŸèƒ½"""
        logger.info("\nğŸ§  æµ‹è¯•è¯­ä¹‰å¢å¼ºåŠŸèƒ½...")
        logger.info("=" * 60)

        try:
            # é‡æ–°åŠ è½½ç›®å½•æ•°æ®
            self.enhancer._load_all_toc_data()
            cache_count = len(self.enhancer._toc_cache)

            logger.info(f"ğŸ“š ç›®å½•ç¼“å­˜æ•°é‡: {cache_count}")

            if cache_count == 0:
                logger.warning("âš ï¸ ç›®å½•ç¼“å­˜ä¸ºç©ºï¼Œè¯­ä¹‰å¢å¼ºåŠŸèƒ½å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œ")
                return False

            # æ˜¾ç¤ºç¼“å­˜è¯¦æƒ…
            for doc_id, toc_data in self.enhancer._toc_cache.items():
                chapters = toc_data.get('chapters', [])
                confidence = toc_data.get('confidence', 0.0)
                status = toc_data.get('status', 'unknown')
                logger.info(f"  â€¢ {doc_id}: {len(chapters)}ä¸ªç« èŠ‚, ç½®ä¿¡åº¦: {confidence:.2f}, çŠ¶æ€: {status}")

            # æµ‹è¯•æŸ¥è¯¢æ„å›¾åˆ†æ
            test_queries = [
                "èµ„æœ¬å……è¶³ç‡ç›¸å…³çš„æŠ¥è¡¨æœ‰å“ªäº›",
                "G01èµ„äº§è´Ÿå€ºé¡¹ç›®ç»Ÿè®¡è¡¨çš„å¡«æŠ¥è¦æ±‚",
                "æ™®æƒ é‡‘èé¢†åŸŸè´·æ¬¾æ¶‰åŠå“ªäº›æŠ¥é€è¡¨"
            ]

            logger.info("\nğŸ” æµ‹è¯•æŸ¥è¯¢æ„å›¾åˆ†æ:")
            for query in test_queries:
                try:
                    intent_result = self.enhancer.analyze_query_intent(query)
                    enabled = intent_result.get('enabled', False)
                    confidence = intent_result.get('confidence', 0.0)
                    keywords = intent_result.get('keywords', [])

                    logger.info(f"  æŸ¥è¯¢: {query}")
                    logger.info(f"    å¯ç”¨: {enabled}, ç½®ä¿¡åº¦: {confidence:.2f}")
                    logger.info(f"    å…³é”®è¯: {keywords[:3] if keywords else 'æ— '}")
                except Exception as e:
                    logger.error(f"  æŸ¥è¯¢å¤±è´¥: {query} - {e}")

            logger.info("âœ… è¯­ä¹‰å¢å¼ºåŠŸèƒ½æµ‹è¯•å®Œæˆ")
            return True

        except Exception as e:
            logger.error(f"âŒ è¯­ä¹‰å¢å¼ºåŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
            return False

    def run_full_pipeline(self, target_docs: Optional[List[str]] = None,
                         test_semantic: bool = True) -> bool:
        """è¿è¡Œå®Œæ•´Pipeline"""
        try:
            # 1. åˆå§‹åŒ–ç»„ä»¶
            if not self.initialize_components():
                return False

            # 2. æ¸…ç†æ—§æ•°æ®
            self.cleanup_old_data()

            # 3. è¿è¡Œæå–Pipeline
            if not self.run_extraction_pipeline(target_docs):
                return False

            # 4. éªŒè¯ç»“æœ
            validation_report = self.validate_extraction_results()

            # 5. ç”ŸæˆæŠ¥å‘Š
            self.generate_summary_report(validation_report)

            # 6. æµ‹è¯•è¯­ä¹‰å¢å¼ºï¼ˆå¯é€‰ï¼‰
            if test_semantic:
                self.test_semantic_enhancement()

            logger.info("\nğŸ‰ Pipelineæ‰§è¡Œå®Œæˆï¼")
            return True

        except Exception as e:
            logger.error(f"âŒ Pipelineæ‰§è¡Œå¤±è´¥: {e}")
            return False


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='CategoryRAGæ–‡æ¡£ç›®å½•æå–è‡ªåŠ¨åŒ–Pipeline')
    parser.add_argument('--pipeline', action='store_true', help='è¿è¡Œå®Œæ•´Pipeline')
    parser.add_argument('--all', action='store_true', help='å¤„ç†æ‰€æœ‰é…ç½®çš„æ–‡æ¡£')
    parser.add_argument('--docs', nargs='+', help='æŒ‡å®šè¦å¤„ç†çš„æ–‡æ¡£ID')
    parser.add_argument('--no-semantic', action='store_true', help='è·³è¿‡è¯­ä¹‰å¢å¼ºæµ‹è¯•')

    args = parser.parse_args()

    if not args.pipeline:
        print("è¯·ä½¿ç”¨ --pipeline å‚æ•°è¿è¡ŒPipeline")
        print("ç¤ºä¾‹: python3 scripts/toc_extraction_pipeline.py --pipeline --all")
        return

    # åˆ›å»ºPipelineå®ä¾‹
    pipeline = TOCExtractionPipeline()

    # ç¡®å®šè¦å¤„ç†çš„æ–‡æ¡£
    target_docs = None
    if args.docs:
        target_docs = args.docs
    elif not args.all:
        print("è¯·æŒ‡å®š --all å¤„ç†æ‰€æœ‰æ–‡æ¡£ï¼Œæˆ–ä½¿ç”¨ --docs æŒ‡å®šç‰¹å®šæ–‡æ¡£")
        return

    # è¿è¡ŒPipeline
    success = pipeline.run_full_pipeline(
        target_docs=target_docs,
        test_semantic=not args.no_semantic
    )

    if success:
        print("\nâœ… Pipelineæ‰§è¡ŒæˆåŠŸï¼")
        sys.exit(0)
    else:
        print("\nâŒ Pipelineæ‰§è¡Œå¤±è´¥ï¼")
        sys.exit(1)


if __name__ == '__main__':
    main()
