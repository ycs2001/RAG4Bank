#!/usr/bin/env python3
"""
CategoryRAGæ–‡æ¡£æ·»åŠ è‡ªåŠ¨åŒ–å·¥ä½œæµ
ä¸€é”®å®Œæˆæ–°æ–‡æ¡£çš„å¤„ç†ã€å‘é‡åŒ–å’Œé›†æˆ
"""

import os
import sys
import time
import json
import yaml
import logging
import argparse
import shutil
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / 'src'))

from src.config import ConfigManager
from src.core.document_preprocessor import DocumentPreprocessor
from src.llm.deepseek_llm import DeepSeekLLM
from src.retrievers import ChromaDBRetriever

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/add_document.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class DocumentAddResult:
    """æ–‡æ¡£æ·»åŠ ç»“æœ"""
    doc_name: str
    file_path: str
    status: str
    chunks_count: int
    collection_id: str
    processing_time: float
    error_message: Optional[str] = None

class DocumentAddWorkflow:
    """æ–‡æ¡£æ·»åŠ å·¥ä½œæµ"""
    
    def __init__(self):
        """åˆå§‹åŒ–å·¥ä½œæµ"""
        self.config_manager = ConfigManager()
        self.project_root = project_root
        self.raw_docs_dir = self.project_root / "data" / "raw_docs"
        self.processed_docs_dir = self.project_root / "data" / "processed_docs"
        self.chunks_dir = self.processed_docs_dir / "chunks"
        self.toc_dir = self.project_root / "data" / "toc"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.raw_docs_dir.mkdir(parents=True, exist_ok=True)
        self.processed_docs_dir.mkdir(parents=True, exist_ok=True)
        self.chunks_dir.mkdir(parents=True, exist_ok=True)
        self.toc_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info("ğŸ“ å·¥ä½œæµç›®å½•ç»“æ„å·²å‡†å¤‡å®Œæˆ")
    
    def add_document(self, file_path: str, collection_config: Optional[Dict] = None) -> DocumentAddResult:
        """
        æ·»åŠ å•ä¸ªæ–‡æ¡£åˆ°ç³»ç»Ÿ
        
        Args:
            file_path: æ–‡æ¡£æ–‡ä»¶è·¯å¾„
            collection_config: é›†åˆé…ç½®ä¿¡æ¯
            
        Returns:
            DocumentAddResult: å¤„ç†ç»“æœ
        """
        start_time = time.time()
        file_path = Path(file_path)
        doc_name = file_path.stem
        
        logger.info(f"ğŸš€ å¼€å§‹å¤„ç†æ–‡æ¡£: {doc_name}")
        
        try:
            # æ­¥éª¤1: å¤åˆ¶æ–‡æ¡£åˆ°raw_docsç›®å½•
            if not file_path.parent == self.raw_docs_dir:
                target_path = self.raw_docs_dir / file_path.name
                shutil.copy2(file_path, target_path)
                logger.info(f"ğŸ“‹ æ–‡æ¡£å·²å¤åˆ¶åˆ°: {target_path}")
                file_path = target_path
            
            # æ­¥éª¤2: æ–‡æ¡£å¤„ç†å’Œåˆ†å—
            chunks_count = self._process_document(file_path)
            
            # æ­¥éª¤3: TOCæå–
            self._extract_toc(file_path)
            
            # æ­¥éª¤4: å‘é‡åŒ–å’Œæ•°æ®åº“æ›´æ–°
            collection_id = self._update_database(doc_name, collection_config)
            
            # æ­¥éª¤5: æ›´æ–°é…ç½®æ–‡ä»¶
            self._update_config(doc_name, collection_id, collection_config)
            
            processing_time = time.time() - start_time
            
            result = DocumentAddResult(
                doc_name=doc_name,
                file_path=str(file_path),
                status="success",
                chunks_count=chunks_count,
                collection_id=collection_id,
                processing_time=processing_time
            )
            
            logger.info(f"âœ… æ–‡æ¡£æ·»åŠ å®Œæˆ: {doc_name} ({processing_time:.2f}ç§’)")
            return result
            
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = str(e)
            logger.error(f"âŒ æ–‡æ¡£æ·»åŠ å¤±è´¥: {doc_name} - {error_msg}")
            
            return DocumentAddResult(
                doc_name=doc_name,
                file_path=str(file_path),
                status="failed",
                chunks_count=0,
                collection_id="",
                processing_time=processing_time,
                error_message=error_msg
            )
    
    def _process_document(self, file_path: Path) -> int:
        """å¤„ç†æ–‡æ¡£å¹¶ç”Ÿæˆåˆ†å—"""
        logger.info(f"ğŸ“„ å¼€å§‹å¤„ç†æ–‡æ¡£: {file_path.name}")
        
        # è°ƒç”¨ç°æœ‰çš„æ–‡æ¡£å¤„ç†å™¨
        sys.path.insert(0, str(self.project_root))
        from document_processor import DocumentProcessingWorkflow

        processor = DocumentProcessingWorkflow(
            input_dir=str(self.raw_docs_dir),
            output_base_dir=str(self.processed_docs_dir)
        )

        # å¤„ç†å•ä¸ªæ–‡æ¡£
        result = processor.process_single_document(str(file_path))
        if result['status'] != 'success':
            raise Exception(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
        
        # ç»Ÿè®¡ç”Ÿæˆçš„åˆ†å—æ•°é‡
        doc_name = file_path.stem
        doc_chunks_dir = self.chunks_dir / doc_name
        
        if doc_chunks_dir.exists():
            chunks_count = len(list(doc_chunks_dir.glob("*.md")))
            logger.info(f"ğŸ“Š ç”Ÿæˆåˆ†å—æ•°é‡: {chunks_count}")
            return chunks_count
        else:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°åˆ†å—ç›®å½•: {doc_chunks_dir}")
            return 0
    
    def _extract_toc(self, file_path: Path):
        """æå–æ–‡æ¡£ç›®å½•ï¼ˆä»…æ”¯æŒPDFå’ŒWordæ–‡æ¡£ï¼‰"""
        file_ext = file_path.suffix.lower()

        # æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ”¯æŒTOCæå–
        if file_ext not in ['.pdf', '.docx', '.doc']:
            logger.info(f"ğŸ“‘ è·³è¿‡TOCæå–: {file_path.name} (ä¸æ”¯æŒçš„æ ¼å¼: {file_ext})")
            logger.info(f"ğŸ’¡ TOCæå–ä»…æ”¯æŒPDFå’ŒWordæ–‡æ¡£æ ¼å¼")
            return

        logger.info(f"ğŸ“‘ å¼€å§‹æå–TOC: {file_path.name}")

        try:
            # è°ƒç”¨TOCæå–è„šæœ¬
            sys.path.insert(0, str(self.project_root / 'scripts'))
            from toc_extraction_pipeline import TOCExtractionPipeline

            toc_pipeline = TOCExtractionPipeline()
            toc_pipeline.process_single_document(str(file_path))

            logger.info(f"âœ… TOCæå–å®Œæˆ: {file_path.name}")

        except Exception as e:
            logger.warning(f"âš ï¸ TOCæå–å¤±è´¥: {e}")
            logger.info(f"ğŸ’¡ TOCæå–å¤±è´¥ä¸ä¼šå½±å“æ–‡æ¡£çš„æ­£å¸¸æ·»åŠ å’Œæ£€ç´¢åŠŸèƒ½")
    
    def _update_database(self, doc_name: str, collection_config: Optional[Dict]) -> str:
        """æ›´æ–°å‘é‡æ•°æ®åº“"""
        logger.info(f"ğŸ—„ï¸ å¼€å§‹æ›´æ–°æ•°æ®åº“: {doc_name}")
        
        # ç¡®å®šé›†åˆID
        if collection_config and 'collection_id' in collection_config:
            collection_id = collection_config['collection_id']
        else:
            # ç”Ÿæˆé»˜è®¤é›†åˆID
            collection_id = doc_name.lower().replace(' ', '_').replace('ã€', '_').replace('ã€‘', '_')
        
        # è°ƒç”¨æ•°æ®åº“æ„å»ºå™¨
        sys.path.insert(0, str(self.project_root))
        from collection_database_builder import CollectionDatabaseBuilder

        builder = CollectionDatabaseBuilder()

        # æ·»åŠ æ–°çš„æ–‡æ¡£æ˜ å°„
        builder.doc_to_collection_mapping[doc_name] = collection_id

        # é‡æ–°æ„å»ºæ•°æ®åº“ï¼ˆè‡ªåŠ¨æ¨¡å¼ï¼‰
        builder.build_auto()
        
        logger.info(f"âœ… æ•°æ®åº“æ›´æ–°å®Œæˆ: {collection_id}")
        return collection_id
    
    def _update_config(self, doc_name: str, collection_id: str, collection_config: Optional[Dict]):
        """æ›´æ–°é…ç½®æ–‡ä»¶"""
        logger.info(f"âš™ï¸ å¼€å§‹æ›´æ–°é…ç½®: {doc_name}")
        
        config_path = self.project_root / "config" / "config.yaml"
        
        # è¯»å–ç°æœ‰é…ç½®
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        # æ›´æ–°ä¸»é¢˜åˆ†ç±»å…³é”®è¯æ˜ å°„
        if 'topic_classification' not in config:
            config['topic_classification'] = {}
        if 'keyword_mapping' not in config['topic_classification']:
            config['topic_classification']['keyword_mapping'] = {}
        
        # æ·»åŠ æ–°é›†åˆçš„å…³é”®è¯
        if collection_config and 'keywords' in collection_config:
            keywords = collection_config['keywords']
        else:
            # ä½¿ç”¨æ–‡æ¡£åä½œä¸ºé»˜è®¤å…³é”®è¯
            keywords = [doc_name, collection_id]
        
        config['topic_classification']['keyword_mapping'][collection_id] = keywords
        
        # æ›´æ–°é›†åˆé…ç½®
        if 'retrieval' not in config:
            config['retrieval'] = {}
        if 'collections' not in config['retrieval']:
            config['retrieval']['collections'] = {}
        
        collection_name = collection_config.get('name', doc_name) if collection_config else doc_name
        collection_desc = collection_config.get('description', f'{doc_name}ç›¸å…³æ–‡æ¡£') if collection_config else f'{doc_name}ç›¸å…³æ–‡æ¡£'
        
        config['retrieval']['collections'][collection_id] = {
            'name': collection_name,
            'description': collection_desc,
            'enabled': True
        }
        
        # ä¿å­˜é…ç½®
        with open(config_path, 'w', encoding='utf-8') as f:
            yaml.dump(config, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… é…ç½®æ›´æ–°å®Œæˆ: {collection_id}")
    
    def interactive_add(self):
        """äº¤äº’å¼æ·»åŠ æ–‡æ¡£"""
        print("ğŸ¯ CategoryRAGæ–‡æ¡£æ·»åŠ å·¥å…·")
        print("=" * 50)
        
        # è·å–æ–‡æ¡£è·¯å¾„
        while True:
            file_path = input("ğŸ“ è¯·è¾“å…¥æ–‡æ¡£è·¯å¾„: ").strip()
            if not file_path:
                print("âŒ è·¯å¾„ä¸èƒ½ä¸ºç©º")
                continue
            
            file_path = Path(file_path)
            if not file_path.exists():
                print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                continue
            
            if file_path.suffix.lower() not in ['.docx', '.doc', '.pdf', '.xlsx', '.xls']:
                print(f"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path.suffix}")
                print(f"ğŸ’¡ æ”¯æŒçš„æ ¼å¼: PDF (.pdf), Word (.docx, .doc), Excel (.xlsx, .xls)")
                continue
            
            break
        
        # è·å–é›†åˆé…ç½®
        print(f"\nğŸ“š é…ç½®æ–‡æ¡£é›†åˆä¿¡æ¯:")
        collection_id = input("é›†åˆID (ç•™ç©ºè‡ªåŠ¨ç”Ÿæˆ): ").strip()
        collection_name = input("é›†åˆåç§°: ").strip()
        collection_desc = input("é›†åˆæè¿°: ").strip()
        keywords_input = input("å…³é”®è¯ (ç”¨é€—å·åˆ†éš”): ").strip()
        
        collection_config = {}
        if collection_id:
            collection_config['collection_id'] = collection_id
        if collection_name:
            collection_config['name'] = collection_name
        if collection_desc:
            collection_config['description'] = collection_desc
        if keywords_input:
            collection_config['keywords'] = [k.strip() for k in keywords_input.split(',')]
        
        # æ‰§è¡Œæ·»åŠ 
        print(f"\nğŸš€ å¼€å§‹æ·»åŠ æ–‡æ¡£...")
        result = self.add_document(str(file_path), collection_config)
        
        # æ˜¾ç¤ºç»“æœ
        print(f"\nğŸ“Š å¤„ç†ç»“æœ:")
        print(f"   çŠ¶æ€: {'âœ… æˆåŠŸ' if result.status == 'success' else 'âŒ å¤±è´¥'}")
        print(f"   æ–‡æ¡£: {result.doc_name}")
        print(f"   åˆ†å—æ•°: {result.chunks_count}")
        print(f"   é›†åˆID: {result.collection_id}")
        print(f"   å¤„ç†æ—¶é—´: {result.processing_time:.2f}ç§’")
        
        if result.error_message:
            print(f"   é”™è¯¯ä¿¡æ¯: {result.error_message}")
        
        if result.status == "success":
            print(f"\nğŸ‰ æ–‡æ¡£æ·»åŠ æˆåŠŸï¼è¯·é‡å¯CategoryRAGç³»ç»Ÿä»¥ä½¿ç”¨æ–°æ–‡æ¡£ã€‚")
            print(f"   é‡å¯å‘½ä»¤: python3 start.py")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="CategoryRAGæ–‡æ¡£æ·»åŠ å·¥å…·")
    parser.add_argument("--file", "-f", help="è¦æ·»åŠ çš„æ–‡æ¡£è·¯å¾„")
    parser.add_argument("--collection-id", help="é›†åˆID")
    parser.add_argument("--collection-name", help="é›†åˆåç§°")
    parser.add_argument("--collection-desc", help="é›†åˆæè¿°")
    parser.add_argument("--keywords", help="å…³é”®è¯ï¼Œç”¨é€—å·åˆ†éš”")
    parser.add_argument("--interactive", "-i", action="store_true", help="äº¤äº’å¼æ¨¡å¼")
    
    args = parser.parse_args()
    
    workflow = DocumentAddWorkflow()
    
    if args.interactive or not args.file:
        workflow.interactive_add()
    else:
        collection_config = {}
        if args.collection_id:
            collection_config['collection_id'] = args.collection_id
        if args.collection_name:
            collection_config['name'] = args.collection_name
        if args.collection_desc:
            collection_config['description'] = args.collection_desc
        if args.keywords:
            collection_config['keywords'] = [k.strip() for k in args.keywords.split(',')]
        
        result = workflow.add_document(args.file, collection_config)
        
        if result.status == "success":
            print(f"âœ… æ–‡æ¡£æ·»åŠ æˆåŠŸ: {result.doc_name}")
        else:
            print(f"âŒ æ–‡æ¡£æ·»åŠ å¤±è´¥: {result.error_message}")
            sys.exit(1)

if __name__ == "__main__":
    main()
